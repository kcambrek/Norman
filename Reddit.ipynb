{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdLEtKgP013F5uoWoTgQa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcambrek/Norman/blob/master/Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Y2XgVaT-Ye",
        "colab_type": "text"
      },
      "source": [
        "#Norman story generator\n",
        "\n",
        "Reddit has a plethora of surprising niche communities. One of my favourites is r/lifeofnorman. The subreddit is full of small stories written by community members on the mundane life of the protagonist Norman.\n",
        "\n",
        "The goal of this notebook is to retrieve posts from the r/lifeofnorman, train a language generator model on these post and generate new stories. The language generator is already trained on a extremely large corpus and only needs to be fine-tuned. For more information on the language model, see https://openai.com/blog/better-language-models/.\n",
        "\n",
        "It is not necessary to collect the data and train the language generator everytime you use this notebook. If you mount your google drive to this notebook, you can save a model checkpoint which can be loaded in later sessions. Therefore, you only need to collect data and train the model once.\n",
        "\n",
        "The syntax of the results seem to be sound, but the content can be a quite off.\n",
        "Some interesting results:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Norman was hungry.\n",
        "He sat down in his favorite chair, and opened his mouth to say something, but couldn't say anything.\n",
        "Then, Norman's mouth started to move.\n",
        "This was it.\n",
        "Norman's mouth was moving.\n",
        "Norman stood up, and started to eat.\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Norman woke up.\n",
        "He had a fever and was worried he might get sick, but he got up and got out of bed.\n",
        "\"I should be careful, I can't sleep tonight.\" Norman went to the kitchen and opened his fridge.\n",
        "Norman's stomach was full.\n",
        "He had to have breakfast.\n",
        "Norman went to bed and got up to go to the bathroom.\n",
        "On his way out he tried to get Norman's medicine.\n",
        "He didn't want to risk getting sick.\n",
        "\"That's okay\" he thought.\n",
        "Norman was in heaven.\n",
        "He had to take his medicine.\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Norman went to the cinema.\n",
        "It was a good one, he thought.\n",
        "He enjoyed the marketing; it made him feel special.\n",
        "He went to the cinema and sat down with a good date.\n",
        "When the date came, the two sat down and enjoyed their popcorn.\n",
        "The date was nice, but not nice enough for Norman.\n",
        "Norman thought it was a little too much, and thought he'd be late.\n",
        "He'd already been through all the events of the day, and he'd go straight to the theatre.\n",
        "Norman decided that he didn't want to be late; he didn't want to be a pest.\n",
        "He also didn't want to be a bother to the hostess.\n",
        "Norman sat down to watch CSI.\n",
        "The date ended and Norman got up to go to bed.\n",
        "He went to sleep, but not before finishing up his popcorn.\n",
        "```\n",
        "\n",
        "\n",
        "And the last one, inspired by https://www.reddit.com/r/lifeofnorman/comments/3ykvsw/norman_considers_suicide/:\n",
        "\n",
        "```\n",
        "Norman considers suicide.\n",
        "Norman recently found himself in situations where he couldn't think of anything to say to the woman at the checkout.\n",
        "Norman couldn't get himself to say anything.\n",
        "Norman decides that he'd rather die than live.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a350X_KJj78Z",
        "colab_type": "text"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wutaqG-FL78v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "926910ec-c52d-48b4-e816-cfd2182a0f27"
      },
      "source": [
        "#install and import gpt2 simple\n",
        "!pip install gpt-2-simple\n",
        "import tensorflow as tf\n",
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.18.2)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=71adf8d47502d2b8af0742df28aa9971be9df65e3f57394f8d4e7f8fdea8eda5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMwuzrccgJcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "001e6e00-8bb4-4bed-c4db-fc9ae847e405"
      },
      "source": [
        "#connect personal google drive to session\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DWVAjCFjVhF",
        "colab_type": "text"
      },
      "source": [
        "#Data Gathering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D6cehAmMhIl",
        "colab_type": "text"
      },
      "source": [
        "We can get reddit data from the Pushshift API. However, there is a limit of how much data one can get in one request. To solve this, we can get the required data in multiple smaller requests. Therefore, we make a list with every month since r/lifeofnorman is founded till now and set the maximum submissions to 500 per month (which seems reasonable for a relatively small subreddit).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTonRSExq5KA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "months = []\n",
        "\n",
        "today = datetime.today()\n",
        "current = datetime(2013, 4, 18)  \n",
        "#for some reason the pushshift api only accepts data arguments in unix time stamp format.\n",
        "while current <= today:\n",
        "    months.append(int(current.replace(tzinfo=timezone.utc).timestamp()))\n",
        "    current += relativedelta(months=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nqSTao-lk8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "results = []\n",
        "for i in range(len(months)-1):\n",
        "  r = requests.get('https://api.pushshift.io/reddit/search/submission/?subreddit=lifeofnorman&after={}&before={}&sort_type=score&sort=desc&size=500'.format(months[i], months[i+1]))\n",
        "  if r.status_code != 200:\n",
        "    print(\"Something went wrong:\", r.status_code)\n",
        "    break\n",
        "  else:\n",
        "    results.append(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zPK_SIzv4cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we are only interested in the submissions that include a story about Norman and not in meta posts.\n",
        "#luckily the title of most of these posts start with 'Norman'\n",
        "\n",
        "submissions = []\n",
        "\n",
        "for response in results:\n",
        "  for submission in response.json()[\"data\"]:\n",
        "    if submission[\"title\"].startswith(\"Norman\"):\n",
        "      submissions.append(submission[\"selftext\"].rstrip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNhhinN3krKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gpt-2 needs to know when a new piece of text starts and ends. \n",
        "formatted_submissions = [\"<|startoftext|> \" + submission.strip() + \" <|endoftext|>\" for submission in submissions]\n",
        "text = \" \".join(formatted_submissions).replace(\"\\n\", \" \").replace(\"\\\\\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn90ivbbiHxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the data to the current session.\n",
        "f = open(\"norman.txt\", \"w\")\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXFnNjOYRiwq",
        "colab_type": "text"
      },
      "source": [
        "#GPT-2 fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNCSD5ASfwEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PziR5c3hR--M",
        "colab_type": "code",
        "outputId": "63b46aef-3fcd-4e86-8205-04cdcf908cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "#specify the model size; check https://github.com/minimaxir/gpt-2-simple for other options.\n",
        "model_name = \"124M\"\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 187Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 79.7Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 194Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 163Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 212Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 118Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 120Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY8dz1zISFWK",
        "colab_type": "code",
        "outputId": "c5ddc402-1ebe-4297-fd39-a3d69a957830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess, \"norman.txt\", steps=1000)   # steps is max number of training steps\n",
        "#every 100 epochs a new sample will be generated to display process."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1558548 tokens\n",
            "Training...\n",
            "[1 | 13.80] loss=3.15 avg=3.15\n",
            "[2 | 18.12] loss=3.56 avg=3.36\n",
            "[3 | 22.46] loss=3.27 avg=3.33\n",
            "[4 | 26.79] loss=3.37 avg=3.34\n",
            "[5 | 31.13] loss=3.24 avg=3.32\n",
            "[6 | 35.48] loss=3.29 avg=3.32\n",
            "[7 | 39.80] loss=3.30 avg=3.31\n",
            "[8 | 44.12] loss=3.09 avg=3.28\n",
            "[9 | 48.49] loss=3.21 avg=3.28\n",
            "[10 | 52.83] loss=2.99 avg=3.25\n",
            "[11 | 57.19] loss=3.04 avg=3.23\n",
            "[12 | 61.56] loss=3.13 avg=3.22\n",
            "[13 | 65.92] loss=3.17 avg=3.21\n",
            "[14 | 70.31] loss=3.16 avg=3.21\n",
            "[15 | 74.69] loss=3.10 avg=3.20\n",
            "[16 | 79.09] loss=2.91 avg=3.18\n",
            "[17 | 83.44] loss=3.02 avg=3.17\n",
            "[18 | 87.83] loss=2.98 avg=3.16\n",
            "[19 | 92.25] loss=2.94 avg=3.15\n",
            "[20 | 96.64] loss=3.16 avg=3.15\n",
            "[21 | 101.04] loss=3.12 avg=3.15\n",
            "[22 | 105.42] loss=3.27 avg=3.15\n",
            "[23 | 109.80] loss=2.93 avg=3.14\n",
            "[24 | 114.20] loss=3.10 avg=3.14\n",
            "[25 | 118.59] loss=3.00 avg=3.13\n",
            "[26 | 123.01] loss=2.88 avg=3.12\n",
            "[27 | 127.41] loss=3.07 avg=3.12\n",
            "[28 | 131.81] loss=2.94 avg=3.11\n",
            "[29 | 136.20] loss=3.15 avg=3.12\n",
            "[30 | 140.59] loss=2.84 avg=3.10\n",
            "[31 | 144.98] loss=2.89 avg=3.10\n",
            "[32 | 149.37] loss=3.03 avg=3.09\n",
            "[33 | 153.78] loss=3.14 avg=3.10\n",
            "[34 | 158.17] loss=3.04 avg=3.09\n",
            "[35 | 162.57] loss=3.05 avg=3.09\n",
            "[36 | 167.00] loss=3.18 avg=3.10\n",
            "[37 | 171.38] loss=2.97 avg=3.09\n",
            "[38 | 175.78] loss=2.84 avg=3.08\n",
            "[39 | 180.19] loss=3.06 avg=3.08\n",
            "[40 | 184.59] loss=2.89 avg=3.08\n",
            "[41 | 188.99] loss=2.90 avg=3.07\n",
            "[42 | 193.36] loss=2.90 avg=3.07\n",
            "[43 | 197.75] loss=2.91 avg=3.06\n",
            "[44 | 202.17] loss=3.07 avg=3.06\n",
            "[45 | 206.55] loss=2.96 avg=3.06\n",
            "[46 | 210.95] loss=2.98 avg=3.06\n",
            "[47 | 215.33] loss=2.93 avg=3.05\n",
            "[48 | 219.72] loss=3.03 avg=3.05\n",
            "[49 | 224.10] loss=3.11 avg=3.06\n",
            "[50 | 228.50] loss=3.17 avg=3.06\n",
            "[51 | 232.91] loss=2.90 avg=3.05\n",
            "[52 | 237.29] loss=3.05 avg=3.05\n",
            "[53 | 241.67] loss=3.09 avg=3.05\n",
            "[54 | 246.08] loss=3.24 avg=3.06\n",
            "[55 | 250.47] loss=3.01 avg=3.06\n",
            "[56 | 254.85] loss=3.10 avg=3.06\n",
            "[57 | 259.26] loss=3.11 avg=3.06\n",
            "[58 | 263.64] loss=3.01 avg=3.06\n",
            "[59 | 268.04] loss=3.08 avg=3.06\n",
            "[60 | 272.44] loss=2.99 avg=3.06\n",
            "[61 | 276.85] loss=3.07 avg=3.06\n",
            "[62 | 281.27] loss=2.95 avg=3.06\n",
            "[63 | 285.67] loss=2.82 avg=3.05\n",
            "[64 | 290.08] loss=2.91 avg=3.05\n",
            "[65 | 294.50] loss=3.05 avg=3.05\n",
            "[66 | 298.89] loss=2.90 avg=3.04\n",
            "[67 | 303.29] loss=2.74 avg=3.04\n",
            "[68 | 307.71] loss=2.67 avg=3.03\n",
            "[69 | 312.09] loss=2.93 avg=3.03\n",
            "[70 | 316.46] loss=2.90 avg=3.03\n",
            "[71 | 320.84] loss=2.89 avg=3.02\n",
            "[72 | 325.24] loss=2.88 avg=3.02\n",
            "[73 | 329.63] loss=2.85 avg=3.02\n",
            "[74 | 334.03] loss=2.90 avg=3.02\n",
            "[75 | 338.43] loss=2.84 avg=3.01\n",
            "[76 | 342.83] loss=2.95 avg=3.01\n",
            "[77 | 347.23] loss=3.12 avg=3.01\n",
            "[78 | 351.64] loss=2.87 avg=3.01\n",
            "[79 | 356.04] loss=2.81 avg=3.01\n",
            "[80 | 360.41] loss=2.88 avg=3.00\n",
            "[81 | 364.80] loss=2.88 avg=3.00\n",
            "[82 | 369.21] loss=2.94 avg=3.00\n",
            "[83 | 373.58] loss=2.95 avg=3.00\n",
            "[84 | 377.99] loss=2.93 avg=3.00\n",
            "[85 | 382.38] loss=2.91 avg=3.00\n",
            "[86 | 386.79] loss=2.90 avg=3.00\n",
            "[87 | 391.18] loss=3.00 avg=3.00\n",
            "[88 | 395.58] loss=2.88 avg=2.99\n",
            "[89 | 399.99] loss=2.83 avg=2.99\n",
            "[90 | 404.38] loss=3.04 avg=2.99\n",
            "[91 | 408.79] loss=2.84 avg=2.99\n",
            "[92 | 413.20] loss=2.80 avg=2.99\n",
            "[93 | 417.61] loss=2.92 avg=2.99\n",
            "[94 | 422.03] loss=2.93 avg=2.98\n",
            "[95 | 426.42] loss=2.93 avg=2.98\n",
            "[96 | 430.83] loss=3.08 avg=2.98\n",
            "[97 | 435.22] loss=2.96 avg=2.98\n",
            "[98 | 439.64] loss=2.82 avg=2.98\n",
            "[99 | 444.05] loss=2.85 avg=2.98\n",
            "[100 | 448.45] loss=2.89 avg=2.98\n",
            "======== SAMPLE 1 ========\n",
            "      *Harmony*      *Harmony*\n",
            "\n",
            "Konami: *B*h* Kanoa: *B*h* Kanoa: *B*h**\n",
            "\n",
            "Konami: *B*h* Kanoa:  Kanoaaaaad! *B*h* Kanoaaaaad!\n",
            "\n",
            "Konami: *B*h* Kanoa: *b*h* Keita: *B*h* Kanoaaaaad! *B*h* Kanoaaaad! *B*h* Kanoaaaaad!\n",
            "\n",
            "Konami: *B*h* Kanoa: *b*h* Keita: *b*h* Kanoaaaaad!\n",
            "\n",
            "Konami: (Kanoaaaaad!)’ \"Harmony\", the voice responded, \"oh, no, I mean the second one. It's not a good time to leave, anyway.\" Kanoaaaad! *B*h* Kanoaaaaad! *B*h* Kanoaaaaad!\n",
            "\n",
            "Konami: “Oh, yes!” the voice replied, \"B*h* Kanoaaaaad!\" and proceeded to go home, not knowing what the other people were doing with it. She sighed and closed the fridge. \n",
            "\n",
            "*End of the interview,” The voice replied, \"Well, this one”  *Harmony* he was *PASTING*! He couldn’t hear the phone, he couldn’t use the bathroom” \n",
            "\n",
            "Koko: *Harmony*\n",
            "\n",
            "Neko: “Oh, and you must be a very beautiful young-nosebleed woman, just like this one!” to the voice in her head from the bathroom. Koko sighed, \"No, I just wanted to make sure I had one.\" she said after another pause. \n",
            "\n",
            "*End of the interview.”\n",
            "\n",
            "Koko: \"Okay, well I guess I’m here to be in your life.” She replied again, with a bit of pride. <|endoftext|> <|startoftext|> <|startoftext|> As he had been out looking for some good news in his local news outlet, Norman had been on the phone as he was getting into car driving.  The receptionist answered his number and his car began to turn. Norman stopped, he felt the rush of driving.  Norman glanced at the radio and watched the news show he had just started.  The receptionist looked at his name in the news.  The caller was a black female voice.  He had forgotten about the local news and had been afraid of strangers. Norman started to talk to her, trying to get her attention.   Then Norman noticed an article there that stated that a manhunt had been launched for a missing person who had been found in their city. It had been called \"The Missing Person.\"  He saw that the name had been changed and that he should not worry about this.  The phone rang again and the receptionist told Norman that something had changed on his phone. Norman looked to see Norman staring at a picture of his missing son, his heart pounding as he recalled his son, Norman. The receptionist gave him the name of his son, Norman. <|endoftext|> <|startoftext|> Norman came home from work the other day and was getting dressed for work the next day. For some reason, a car was coming up behind him, and it was a brown Ford F-150 with a sticker in the front of it. It was said to be the same color as the one in the rear window of the car.  \"Oh, I'll see you in the news next time\" Norman thought, \"what could there possibly be?\" <|endoftext|> <|startoftext|> One morning in the morning, Norman got up from work to watch CSI: Miami.  He walked up to the window and walked into the office. There, Norman had found a CSI documentary on TV, that he could see through the window - a CSI documentary from the library where Norman was attending.  \"Hey there! Where can we get you one of those CSI DVDs?\"  Norman found that there were CSI DVDs on his shelf. He stood at the desk, and he opened the DVD player of CSI: Miami.  For the first time in his life, he could picture this CSI movie. He stood there for several seconds, thinking, \"I must have seen this one!\" <|endoftext|> <|startoftext|> Norman went home and made breakfast with his cat Norman. He enjoyed talking to Norman, but this time he heard Norman, Norman, Norman was asleep in his apartment.  Norman\n",
            "\n",
            "[101 | 472.19] loss=2.57 avg=2.97\n",
            "[102 | 476.59] loss=2.96 avg=2.97\n",
            "[103 | 481.02] loss=2.76 avg=2.97\n",
            "[104 | 485.40] loss=2.85 avg=2.97\n",
            "[105 | 489.77] loss=3.48 avg=2.97\n",
            "[106 | 494.17] loss=2.66 avg=2.97\n",
            "[107 | 498.56] loss=2.87 avg=2.97\n",
            "[108 | 502.98] loss=2.98 avg=2.97\n",
            "[109 | 507.35] loss=3.07 avg=2.97\n",
            "[110 | 511.74] loss=3.01 avg=2.97\n",
            "[111 | 516.14] loss=3.00 avg=2.97\n",
            "[112 | 520.53] loss=2.90 avg=2.97\n",
            "[113 | 524.92] loss=2.75 avg=2.97\n",
            "[114 | 529.29] loss=2.77 avg=2.96\n",
            "[115 | 533.67] loss=2.88 avg=2.96\n",
            "[116 | 538.06] loss=2.59 avg=2.96\n",
            "[117 | 542.46] loss=2.66 avg=2.95\n",
            "[118 | 546.84] loss=2.92 avg=2.95\n",
            "[119 | 551.24] loss=2.97 avg=2.95\n",
            "[120 | 555.63] loss=2.93 avg=2.95\n",
            "[121 | 560.02] loss=2.82 avg=2.95\n",
            "[122 | 564.41] loss=2.79 avg=2.95\n",
            "[123 | 568.81] loss=3.00 avg=2.95\n",
            "[124 | 573.20] loss=2.86 avg=2.95\n",
            "[125 | 577.60] loss=2.86 avg=2.95\n",
            "[126 | 582.02] loss=2.86 avg=2.94\n",
            "[127 | 586.40] loss=2.87 avg=2.94\n",
            "[128 | 590.79] loss=2.87 avg=2.94\n",
            "[129 | 595.17] loss=2.75 avg=2.94\n",
            "[130 | 599.56] loss=2.86 avg=2.94\n",
            "[131 | 603.93] loss=2.75 avg=2.94\n",
            "[132 | 608.31] loss=2.90 avg=2.94\n",
            "[133 | 612.74] loss=2.86 avg=2.93\n",
            "[134 | 617.12] loss=2.82 avg=2.93\n",
            "[135 | 621.53] loss=2.76 avg=2.93\n",
            "[136 | 625.91] loss=3.04 avg=2.93\n",
            "[137 | 630.29] loss=2.77 avg=2.93\n",
            "[138 | 634.68] loss=2.80 avg=2.93\n",
            "[139 | 639.07] loss=2.69 avg=2.93\n",
            "[140 | 643.45] loss=2.81 avg=2.92\n",
            "[141 | 647.85] loss=2.98 avg=2.92\n",
            "[142 | 652.21] loss=3.07 avg=2.93\n",
            "[143 | 656.61] loss=2.93 avg=2.93\n",
            "[144 | 661.01] loss=2.88 avg=2.93\n",
            "[145 | 665.40] loss=2.96 avg=2.93\n",
            "[146 | 669.78] loss=2.81 avg=2.92\n",
            "[147 | 674.18] loss=2.82 avg=2.92\n",
            "[148 | 678.61] loss=2.82 avg=2.92\n",
            "[149 | 683.00] loss=2.75 avg=2.92\n",
            "[150 | 687.42] loss=2.80 avg=2.92\n",
            "[151 | 691.81] loss=2.82 avg=2.92\n",
            "[152 | 696.20] loss=2.89 avg=2.92\n",
            "[153 | 700.60] loss=2.81 avg=2.92\n",
            "[154 | 704.99] loss=3.00 avg=2.92\n",
            "[155 | 709.38] loss=2.82 avg=2.92\n",
            "[156 | 713.78] loss=2.89 avg=2.92\n",
            "[157 | 718.17] loss=2.72 avg=2.91\n",
            "[158 | 722.57] loss=2.69 avg=2.91\n",
            "[159 | 727.00] loss=2.74 avg=2.91\n",
            "[160 | 731.42] loss=2.76 avg=2.91\n",
            "[161 | 735.81] loss=2.74 avg=2.90\n",
            "[162 | 740.20] loss=2.82 avg=2.90\n",
            "[163 | 744.57] loss=2.85 avg=2.90\n",
            "[164 | 748.95] loss=2.84 avg=2.90\n",
            "[165 | 753.35] loss=3.04 avg=2.90\n",
            "[166 | 757.72] loss=2.82 avg=2.90\n",
            "[167 | 762.13] loss=2.89 avg=2.90\n",
            "[168 | 766.50] loss=2.71 avg=2.90\n",
            "[169 | 770.88] loss=2.87 avg=2.90\n",
            "[170 | 775.26] loss=2.79 avg=2.90\n",
            "[171 | 779.65] loss=3.00 avg=2.90\n",
            "[172 | 784.07] loss=2.93 avg=2.90\n",
            "[173 | 788.47] loss=2.96 avg=2.90\n",
            "[174 | 792.89] loss=2.84 avg=2.90\n",
            "[175 | 797.26] loss=2.74 avg=2.90\n",
            "[176 | 801.64] loss=2.86 avg=2.90\n",
            "[177 | 806.05] loss=2.88 avg=2.90\n",
            "[178 | 810.43] loss=2.94 avg=2.90\n",
            "[179 | 814.83] loss=2.90 avg=2.90\n",
            "[180 | 819.22] loss=2.92 avg=2.90\n",
            "[181 | 823.62] loss=3.07 avg=2.90\n",
            "[182 | 827.99] loss=2.77 avg=2.90\n",
            "[183 | 832.38] loss=2.96 avg=2.90\n",
            "[184 | 836.79] loss=2.77 avg=2.90\n",
            "[185 | 841.17] loss=3.07 avg=2.90\n",
            "[186 | 845.55] loss=2.87 avg=2.90\n",
            "[187 | 849.94] loss=2.86 avg=2.90\n",
            "[188 | 854.34] loss=2.81 avg=2.90\n",
            "[189 | 858.74] loss=2.92 avg=2.90\n",
            "[190 | 863.13] loss=2.94 avg=2.90\n",
            "[191 | 867.52] loss=2.78 avg=2.90\n",
            "[192 | 871.92] loss=3.17 avg=2.90\n",
            "[193 | 876.30] loss=2.72 avg=2.90\n",
            "[194 | 880.71] loss=2.71 avg=2.90\n",
            "[195 | 885.09] loss=2.79 avg=2.89\n",
            "[196 | 889.49] loss=2.95 avg=2.90\n",
            "[197 | 893.91] loss=2.84 avg=2.89\n",
            "[198 | 898.30] loss=2.75 avg=2.89\n",
            "[199 | 902.69] loss=2.88 avg=2.89\n",
            "[200 | 907.09] loss=2.64 avg=2.89\n",
            "======== SAMPLE 1 ========\n",
            "> *If you've had it at all, and have noticed any problems, I'd like to be sure your computer was working properly.* She turned around and walked to the bus and put the ticket into it, leaving the rest for now. <|endoftext|> <|startoftext|> It was a Saturday afternoon in June. Norman had not been to the office since last fall, because the school day earlier had been cancelled. A friend of his, Norman, had taken the bus, and Norman had a few more hours to get there before starting to panic. He went straight for the office desk and looked around. It was empty and empty. Norman was getting ready to go out. He went inside and turned on the TV, hoping the people around him didn't think he was taking too much TV to watch the news.  He was glad to be there when somebody else did. Norman stood in front of him, holding the TV, a small smile on his face. He didn't want to be late. Maybe he'd talk to an accountant today.  Just as Norman was preparing to leave and get out in time for the TV show, Norman noticed a girl behind him. She looked nervous, and asked him who he was and where he was from. Norman knew he was a little nervous and wanted to make sure he was as friendly as he could. He wanted to avoid being rude, and to make sure she understood. His nerves started to turn when he noticed her face: she looked at him and said, \"Your father is a little... old.\"  Norman was confused. He had been an employee there a year ago. He had been asked to leave on a Sunday afternoon, but he had already left work early. Did she mean something wrong, or did she mean something right? He didn't really know. He decided to go over to her workplace. Norman walked to the exit of the building, and stepped inside.  He waited in the elevator, then started to his feet, feeling the soft muscles on his feet. He couldn't help but stretch in them. He felt like a baby.   As Norman sat down on the floor, holding his watch, he heard a woman from a nearby office break into a yawn and go outside. Norman turned on the TV and watched as the secretary made an awkward face behind her desk.  Her smile slowly faded and she turned her attention back to Norman. It was Norman. The secretary was at her desk, talking. Norman was a little embarrassed. He just didn't want to look stupid again. <|endoftext|> <|startoftext|> The day came to Norman Norman decided to give one more chance at work, and that is by signing up for our newsletter.  He wanted to try something new, and he thought about his coworker's birthday party.  It was a great idea, but Norman thought about it at the thought that some day he would have to make do with nothing, and the excitement from it and the memories of Norman that would give him.  A few days later he came back from lunch, and he decided to enjoy the experience without having to make much of an effort. Norman started at one room only, and made his way to a room on the opposite side of the room, and noticed someone holding a small cake that was decorated with the letters \"D\" and \"S.\"  Norman couldn't even imagine his feelings. <|endoftext|> <|startoftext|> Norman was going for a walk through a park when he noticed a car he hadn’t seen before - the bus. The guy had an interesting story, but they all spoke a different language, and their opinions and motivations were completely different. Norman pulled himself up off the park and got all his information about the bus while he waited. He decided the first thing he wanted to know was whether or not the bus would be open anytime of the week.   'No', Norman replied, 'it’s Saturday. No bus opens Saturdays.”  The guy looked at Norman with a very excited expression. Norman looked down at his wrist as he looked around. It was Saturday?  \"Yes,\" Norman replied, \"but I don’t want to be rude.\"  Norman nodded vigorously. In all honesty, he’d never say that,’ but it wasn’t like his wrist was broken. He felt like he owed the man a visit. <|endoftext|> <|startoftext|> It was Tuesday night. Norman went to the grocery store, but there was no place for him to go. The owner told him that he had an address. Norman sat down and put the groceries in the trunk and checked the time on his watch. <|endoftext|> <|startoftext|> Norman was driving home from work. After work he saw Norman parked in the rear of the car.  Norman was going to park on his side, but he\n",
            "\n",
            "[201 | 928.64] loss=2.81 avg=2.89\n",
            "[202 | 933.03] loss=2.94 avg=2.89\n",
            "[203 | 937.41] loss=2.92 avg=2.89\n",
            "[204 | 941.81] loss=2.76 avg=2.89\n",
            "[205 | 946.21] loss=2.68 avg=2.89\n",
            "[206 | 950.60] loss=2.93 avg=2.89\n",
            "[207 | 954.99] loss=2.76 avg=2.89\n",
            "[208 | 959.39] loss=2.81 avg=2.88\n",
            "[209 | 963.78] loss=2.79 avg=2.88\n",
            "[210 | 968.18] loss=2.80 avg=2.88\n",
            "[211 | 972.57] loss=2.88 avg=2.88\n",
            "[212 | 976.96] loss=2.83 avg=2.88\n",
            "[213 | 981.35] loss=2.91 avg=2.88\n",
            "[214 | 985.76] loss=2.84 avg=2.88\n",
            "[215 | 990.14] loss=2.84 avg=2.88\n",
            "[216 | 994.51] loss=2.69 avg=2.88\n",
            "[217 | 998.92] loss=2.74 avg=2.88\n",
            "[218 | 1003.30] loss=2.67 avg=2.87\n",
            "[219 | 1007.70] loss=2.66 avg=2.87\n",
            "[220 | 1012.08] loss=2.74 avg=2.87\n",
            "[221 | 1016.46] loss=2.86 avg=2.87\n",
            "[222 | 1020.84] loss=2.87 avg=2.87\n",
            "[223 | 1025.26] loss=2.72 avg=2.87\n",
            "[224 | 1029.64] loss=2.80 avg=2.87\n",
            "[225 | 1034.02] loss=3.06 avg=2.87\n",
            "[226 | 1038.41] loss=2.76 avg=2.87\n",
            "[227 | 1042.82] loss=2.88 avg=2.87\n",
            "[228 | 1047.22] loss=2.75 avg=2.87\n",
            "[229 | 1051.60] loss=2.83 avg=2.87\n",
            "[230 | 1055.98] loss=2.78 avg=2.87\n",
            "[231 | 1060.38] loss=2.78 avg=2.87\n",
            "[232 | 1064.76] loss=2.57 avg=2.86\n",
            "[233 | 1069.19] loss=2.88 avg=2.86\n",
            "[234 | 1073.60] loss=2.90 avg=2.86\n",
            "[235 | 1077.99] loss=2.72 avg=2.86\n",
            "[236 | 1082.37] loss=2.93 avg=2.86\n",
            "[237 | 1086.77] loss=2.72 avg=2.86\n",
            "[238 | 1091.19] loss=2.75 avg=2.86\n",
            "[239 | 1095.61] loss=2.83 avg=2.86\n",
            "[240 | 1100.00] loss=2.94 avg=2.86\n",
            "[241 | 1104.41] loss=2.87 avg=2.86\n",
            "[242 | 1108.80] loss=2.64 avg=2.86\n",
            "[243 | 1113.21] loss=2.66 avg=2.86\n",
            "[244 | 1117.60] loss=2.74 avg=2.85\n",
            "[245 | 1122.00] loss=2.84 avg=2.85\n",
            "[246 | 1126.36] loss=2.83 avg=2.85\n",
            "[247 | 1130.75] loss=3.01 avg=2.86\n",
            "[248 | 1135.15] loss=2.91 avg=2.86\n",
            "[249 | 1139.55] loss=2.78 avg=2.86\n",
            "[250 | 1143.93] loss=2.85 avg=2.86\n",
            "[251 | 1148.33] loss=2.74 avg=2.85\n",
            "[252 | 1152.72] loss=2.78 avg=2.85\n",
            "[253 | 1157.11] loss=2.73 avg=2.85\n",
            "[254 | 1161.52] loss=2.79 avg=2.85\n",
            "[255 | 1165.90] loss=2.59 avg=2.85\n",
            "[256 | 1170.29] loss=2.66 avg=2.85\n",
            "[257 | 1174.69] loss=2.73 avg=2.85\n",
            "[258 | 1179.10] loss=2.95 avg=2.85\n",
            "[259 | 1183.49] loss=2.65 avg=2.84\n",
            "[260 | 1187.87] loss=2.73 avg=2.84\n",
            "[261 | 1192.25] loss=2.64 avg=2.84\n",
            "[262 | 1196.65] loss=2.91 avg=2.84\n",
            "[263 | 1201.04] loss=2.76 avg=2.84\n",
            "[264 | 1205.43] loss=2.85 avg=2.84\n",
            "[265 | 1209.81] loss=2.62 avg=2.84\n",
            "[266 | 1214.22] loss=2.75 avg=2.84\n",
            "[267 | 1218.61] loss=3.00 avg=2.84\n",
            "[268 | 1223.01] loss=2.95 avg=2.84\n",
            "[269 | 1227.43] loss=2.63 avg=2.84\n",
            "[270 | 1231.83] loss=2.82 avg=2.84\n",
            "[271 | 1236.22] loss=2.56 avg=2.83\n",
            "[272 | 1240.61] loss=2.78 avg=2.83\n",
            "[273 | 1245.02] loss=3.12 avg=2.84\n",
            "[274 | 1249.39] loss=2.76 avg=2.84\n",
            "[275 | 1253.79] loss=2.73 avg=2.84\n",
            "[276 | 1258.19] loss=2.79 avg=2.83\n",
            "[277 | 1262.60] loss=2.79 avg=2.83\n",
            "[278 | 1266.98] loss=2.76 avg=2.83\n",
            "[279 | 1271.38] loss=2.86 avg=2.83\n",
            "[280 | 1275.76] loss=2.70 avg=2.83\n",
            "[281 | 1280.15] loss=2.66 avg=2.83\n",
            "[282 | 1284.53] loss=2.78 avg=2.83\n",
            "[283 | 1288.95] loss=2.92 avg=2.83\n",
            "[284 | 1293.37] loss=2.63 avg=2.83\n",
            "[285 | 1297.76] loss=2.75 avg=2.83\n",
            "[286 | 1302.18] loss=2.82 avg=2.83\n",
            "[287 | 1306.58] loss=2.88 avg=2.83\n",
            "[288 | 1311.00] loss=2.89 avg=2.83\n",
            "[289 | 1315.42] loss=2.52 avg=2.83\n",
            "[290 | 1319.80] loss=2.52 avg=2.82\n",
            "[291 | 1324.18] loss=2.74 avg=2.82\n",
            "[292 | 1328.59] loss=2.78 avg=2.82\n",
            "[293 | 1332.98] loss=2.64 avg=2.82\n",
            "[294 | 1337.38] loss=2.72 avg=2.82\n",
            "[295 | 1341.80] loss=2.83 avg=2.82\n",
            "[296 | 1346.20] loss=2.83 avg=2.82\n",
            "[297 | 1350.61] loss=2.63 avg=2.82\n",
            "[298 | 1355.01] loss=2.94 avg=2.82\n",
            "[299 | 1359.39] loss=2.73 avg=2.82\n",
            "[300 | 1363.76] loss=2.76 avg=2.82\n",
            "======== SAMPLE 1 ========\n",
            "    \"Hi!\" gasped Norman as he glanced down at his keyboard.   \"I'm Norman. Sorry about that. I've forgotten all about my birthday. Oh, my.\"   \"What is it?\"  \"Well I'm going to try again and get two new socks.\"    \"Hey, Norman!\"  \"Thank you, my socks are very warm in here.\"    \"Oh dear, did you ever try this stuff for me? That was so... you know, fun!\"   Norman let out *a sigh of relief*, turned around, and headed to the door.    The door swung open and Norman suddenly felt hungry!    A look of worry crossed Norman's face as he looked down at the sock.  A quick thought of a banana salad followed by a quick sip turned Norman off to the notion of an extra slice.  As he stood up from the table, Norman felt the need to rub the wrong way.  He tried rubbing his left thigh and left leg, but he couldn't quite get rid of them.  Eventually Norman stopped.  Norman looked at his cat, turned his back, and went back to his desk.    Norman opened his phone and dialed 911.  Norman remembered when he was little. He told him he could fix it for him. *Maybe*, Norman thought to himself. <|endoftext|> <|startoftext|> On his way home from work, Norman felt like he'd finished a long, tedious work and needed something new to do.  He picked up his favorite snack, a single bag of crisps.  Norman turned on his television and watched CSI. The TV in the corner of his television room lit up with a small and glowing flash of blue light and the theme song was \"Happy Birthday, Norman\".  But when he was on the last episode he remembered, it looked like New England.  But no, he remembered, he'd been in New England for a long time.  He walked to home-made chicken sandwiches at the local sandwich shop.  He stopped and thought about the last season.  He was a little sad about the last time he went to visit New York City.  But no thanks.  He smiled and placed all his food into his lap.  When the next episode came out he was aghast to look at his new sweater instead.  In disbelief he remembered that he'd ordered this new sweater one year prior because he was on a mission and he wanted to see New York City.  But instead that year he decided to shop in the other department store and see if they had some new clothing and shoes.  He walked through the shops and found a nice selection to his liking.   It was a warm winter day and Norman had to go to the park to watch the autumn weather.  As he was walking back toward his house he noticed he was in a hurry and wanted to go back before the storm hit.  What could it be?   \"My son wants to buy a sweater\", Norman thought.  Norman looked into his wife's eyes and saw a sweater he'd never bought before.  He realized she'd probably never seen it and hadn't noticed anything.  But Norman remembered that, back in the day he'd bought a sweater from a friend's house and it was still on the top shelf when he looked outside.  His heart was racing and he was thinking about how they might try something new and expensive this year.  He'd bought a sweater this season before.  Norman tried to make up for his failure with a new sweater he'd originally bought and it was still on the top shelf.  Norman couldn't believe his luck and decided to buy another sweater from a friend.  At the park he was able to look around the park and see only one sweater at the bottom shelf.  Then he remembered there was already a sweater on clearance at the store on the other side of the town.  He thought it was silly of Norman to buy another sweater.  At home he grabbed his keys, placed them under the couch, and headed home to see if he could find another sweater.  As he was walking back to his house he noticed he was running late and decided to eat lunch.  Suddenly he saw something strange coming towards him.  He saw a cat running towards him and suddenly he was running.  Norman went to get his food, but then he remembered he never ate a cat before. He checked his food and he saw that bacon, ham and cheese sandwiches were on sale.  Norman was worried and tried to calm down and think how much he wanted to go and get something new.  Then he remembered he didn't buy cheese slices anyway.  He didn't want to eat any more cheese slices.  But as Norman got home he thought about bacon and cheese sandwiches and he didn't want to eat any cheese slices either.  The day went on and Norman ate a bit too much.  At\n",
            "\n",
            "[301 | 1385.06] loss=2.86 avg=2.82\n",
            "[302 | 1389.43] loss=2.82 avg=2.82\n",
            "[303 | 1393.83] loss=2.69 avg=2.82\n",
            "[304 | 1398.23] loss=2.69 avg=2.81\n",
            "[305 | 1402.61] loss=2.95 avg=2.82\n",
            "[306 | 1406.99] loss=2.66 avg=2.81\n",
            "[307 | 1411.37] loss=2.83 avg=2.81\n",
            "[308 | 1415.76] loss=2.53 avg=2.81\n",
            "[309 | 1420.16] loss=2.67 avg=2.81\n",
            "[310 | 1424.55] loss=2.96 avg=2.81\n",
            "[311 | 1428.96] loss=2.56 avg=2.81\n",
            "[312 | 1433.33] loss=2.69 avg=2.81\n",
            "[313 | 1437.73] loss=2.59 avg=2.80\n",
            "[314 | 1442.15] loss=2.71 avg=2.80\n",
            "[315 | 1446.52] loss=2.84 avg=2.80\n",
            "[316 | 1450.89] loss=2.64 avg=2.80\n",
            "[317 | 1455.30] loss=2.83 avg=2.80\n",
            "[318 | 1459.70] loss=2.80 avg=2.80\n",
            "[319 | 1464.08] loss=2.64 avg=2.80\n",
            "[320 | 1468.49] loss=2.75 avg=2.80\n",
            "[321 | 1472.90] loss=2.68 avg=2.80\n",
            "[322 | 1477.29] loss=2.66 avg=2.80\n",
            "[323 | 1481.67] loss=2.91 avg=2.80\n",
            "[324 | 1486.06] loss=2.69 avg=2.80\n",
            "[325 | 1490.44] loss=2.63 avg=2.80\n",
            "[326 | 1494.82] loss=2.74 avg=2.80\n",
            "[327 | 1499.20] loss=2.52 avg=2.79\n",
            "[328 | 1503.57] loss=2.82 avg=2.79\n",
            "[329 | 1507.97] loss=2.86 avg=2.79\n",
            "[330 | 1512.36] loss=2.71 avg=2.79\n",
            "[331 | 1516.75] loss=2.77 avg=2.79\n",
            "[332 | 1521.12] loss=2.81 avg=2.79\n",
            "[333 | 1525.52] loss=2.94 avg=2.79\n",
            "[334 | 1529.92] loss=2.49 avg=2.79\n",
            "[335 | 1534.33] loss=2.30 avg=2.79\n",
            "[336 | 1538.71] loss=2.75 avg=2.79\n",
            "[337 | 1543.12] loss=2.65 avg=2.78\n",
            "[338 | 1547.54] loss=2.67 avg=2.78\n",
            "[339 | 1551.93] loss=2.78 avg=2.78\n",
            "[340 | 1556.34] loss=2.61 avg=2.78\n",
            "[341 | 1560.75] loss=2.76 avg=2.78\n",
            "[342 | 1565.15] loss=2.88 avg=2.78\n",
            "[343 | 1569.56] loss=2.48 avg=2.78\n",
            "[344 | 1573.97] loss=2.67 avg=2.78\n",
            "[345 | 1578.35] loss=2.75 avg=2.78\n",
            "[346 | 1582.75] loss=2.51 avg=2.77\n",
            "[347 | 1587.18] loss=2.83 avg=2.78\n",
            "[348 | 1591.58] loss=2.55 avg=2.77\n",
            "[349 | 1596.00] loss=2.86 avg=2.77\n",
            "[350 | 1600.41] loss=2.66 avg=2.77\n",
            "[351 | 1604.82] loss=2.73 avg=2.77\n",
            "[352 | 1609.22] loss=3.32 avg=2.78\n",
            "[353 | 1613.62] loss=2.78 avg=2.78\n",
            "[354 | 1618.03] loss=2.72 avg=2.78\n",
            "[355 | 1622.45] loss=2.86 avg=2.78\n",
            "[356 | 1626.86] loss=2.73 avg=2.78\n",
            "[357 | 1631.26] loss=2.55 avg=2.78\n",
            "[358 | 1635.71] loss=2.58 avg=2.77\n",
            "[359 | 1640.10] loss=2.43 avg=2.77\n",
            "[360 | 1644.52] loss=2.60 avg=2.77\n",
            "[361 | 1648.93] loss=2.77 avg=2.77\n",
            "[362 | 1653.34] loss=2.77 avg=2.77\n",
            "[363 | 1657.77] loss=2.54 avg=2.77\n",
            "[364 | 1662.18] loss=2.74 avg=2.77\n",
            "[365 | 1666.59] loss=2.73 avg=2.77\n",
            "[366 | 1670.98] loss=2.71 avg=2.76\n",
            "[367 | 1675.39] loss=2.91 avg=2.77\n",
            "[368 | 1679.81] loss=2.72 avg=2.77\n",
            "[369 | 1684.22] loss=2.92 avg=2.77\n",
            "[370 | 1688.62] loss=2.58 avg=2.77\n",
            "[371 | 1693.01] loss=2.74 avg=2.77\n",
            "[372 | 1697.39] loss=2.56 avg=2.76\n",
            "[373 | 1701.76] loss=2.74 avg=2.76\n",
            "[374 | 1706.15] loss=2.71 avg=2.76\n",
            "[375 | 1710.54] loss=2.55 avg=2.76\n",
            "[376 | 1714.94] loss=2.87 avg=2.76\n",
            "[377 | 1719.33] loss=2.85 avg=2.76\n",
            "[378 | 1723.74] loss=2.66 avg=2.76\n",
            "[379 | 1728.12] loss=2.81 avg=2.76\n",
            "[380 | 1732.51] loss=2.74 avg=2.76\n",
            "[381 | 1736.90] loss=2.85 avg=2.76\n",
            "[382 | 1741.31] loss=2.82 avg=2.76\n",
            "[383 | 1745.70] loss=2.64 avg=2.76\n",
            "[384 | 1750.11] loss=2.72 avg=2.76\n",
            "[385 | 1754.50] loss=2.61 avg=2.76\n",
            "[386 | 1758.92] loss=2.67 avg=2.76\n",
            "[387 | 1763.28] loss=2.57 avg=2.76\n",
            "[388 | 1767.66] loss=2.79 avg=2.76\n",
            "[389 | 1772.04] loss=2.76 avg=2.76\n",
            "[390 | 1776.43] loss=3.16 avg=2.76\n",
            "[391 | 1780.85] loss=2.67 avg=2.76\n",
            "[392 | 1785.24] loss=2.56 avg=2.76\n",
            "[393 | 1789.65] loss=2.73 avg=2.76\n",
            "[394 | 1794.04] loss=2.93 avg=2.76\n",
            "[395 | 1798.46] loss=2.66 avg=2.76\n",
            "[396 | 1802.84] loss=2.93 avg=2.76\n",
            "[397 | 1807.24] loss=2.75 avg=2.76\n",
            "[398 | 1811.61] loss=2.72 avg=2.76\n",
            "[399 | 1816.02] loss=2.75 avg=2.76\n",
            "[400 | 1820.40] loss=2.74 avg=2.76\n",
            "======== SAMPLE 1 ========\n",
            "ig, but he never made it into the office to work on his own. It was as he went to sleep that the sun set. <|endoftext|> <|startoftext|>  Norman sat on the sofa, as he had for a long, long time. He didn't want to work too hard or too little time, which made the sofa warmer again.   Norman thought about how he felt on that new night where his son didn't come to visit but he just had to stay warm, and all the friends he met on his way to the beach until the next week. If he had stayed that way, he might have had a weekend to prepare for the weekend. That was his hope, though.   Norman was happy that he was having as much fun as he could over tonight's episode of CSI, so if the show's ending was going to be a surprise, he would probably have enough excitement for today to be prepared. He tried to remember where he had been but no one could have had any idea.  He watched through the TV, and the show's beginning. It was a commercial break, and Norman had planned something special for himself. When he opened the show's episodes, he was still expecting the episode to be on. He wanted to forget all about it, but the end of the episode was already over. <|endoftext|> <|startoftext|> Norman had been thinking about taking off his shoes when he decided that it would be a good idea to get some fresh air in his sneakers.  He hadn't been particularly good with cold weathers or humidity lately since it started, so Norman's feet had felt very dry this winter. Though it was only a few degrees higher than it was last week, it felt more pleasant than dry. Norman sighed, and felt relieved that his sneeze was over. *And now I can go for a walk*, he thought.   Norman walked down the stairs to his room, and he was feeling pretty good. *I'm so happy that I'm actually out all day,* he thought.   He thought about his son, and about how he would never have liked going out for a walk. However, he also decided he would take it as an experiment and enjoy himself, when he got home. Norman sat down on the couch, and Norman began to work on his sock. Norman thought about his son until it was too late, for he hadn't seen his son in quite a while. After a few rounds of cereal, he decided on a new sock.  Norman slipped his toes into the sock, and as his fingers dug into his sock, he found that he had left a little bit of a bit on Norman.  He sighed, then rubbed his eyes and thought of how warm it would be when his son would come back. <|endoftext|> <|startoftext|> Norman had been sitting in his cubicle watching a commercial for a local television network, watching Norman's cat, Norman, come and bark at a squirrel. Norman knew the squirrels would bark when people walked by, and he knew that if the squirrel would bark that squirrel would have been in his direction. As a young, Norman watched some videos of squirrels that barked at squirrels, and he imagined them playing a game of fetch, and he would bark at them as they would bark at him. That day, Norman was in the middle of his work day, and he felt he should have a chance to do something, and decided he should go out to enjoy his work. He walked down the hall, and saw Norman, his cubicle mate. Norman's eyes widened again as he saw that Norman had left a little snow on his floor. Norman was happy, and it got his heart going. So he walked over to his cubicle, and Norman sat down in a chair with his cat. The cat was happily curled up on the grass, and Norman eagerly awaited his return. Norman smiled as Norman was back at work, and felt that it was nice to be back to his little cubicle mate. As he walked past the cubicles and the squirrels, the squirrels barked at Norman, and a young man, Norman. Norman thought it looked very nice for a squirrel. <|endoftext|> <|startoftext|> Norman arrived home from work. He checked his watch, then he noticed his usual time was past 2035. He went to his alarm clock, and it was ticking away at 7:53.  He sat down for the little breakfast he had had for the last three days, and Norman picked up a banana, put it in Norman's lap, and took a quick bite.  Norman made himself coffee for his coffee. Norman thought about how his cat would bark at him if he didn't bark at him. What a lovely feeling. Norman then watched his work at work from the window and got ready to go to bed. <|endof\n",
            "\n",
            "[401 | 1841.89] loss=2.46 avg=2.76\n",
            "[402 | 1846.28] loss=2.69 avg=2.76\n",
            "[403 | 1850.68] loss=2.59 avg=2.75\n",
            "[404 | 1855.06] loss=2.79 avg=2.75\n",
            "[405 | 1859.47] loss=2.78 avg=2.75\n",
            "[406 | 1863.87] loss=2.85 avg=2.76\n",
            "[407 | 1868.27] loss=2.76 avg=2.76\n",
            "[408 | 1872.65] loss=2.51 avg=2.75\n",
            "[409 | 1877.02] loss=2.74 avg=2.75\n",
            "[410 | 1881.42] loss=2.69 avg=2.75\n",
            "[411 | 1885.81] loss=2.81 avg=2.75\n",
            "[412 | 1890.18] loss=2.69 avg=2.75\n",
            "[413 | 1894.58] loss=2.58 avg=2.75\n",
            "[414 | 1898.96] loss=2.79 avg=2.75\n",
            "[415 | 1903.35] loss=2.76 avg=2.75\n",
            "[416 | 1907.73] loss=2.55 avg=2.75\n",
            "[417 | 1912.16] loss=2.84 avg=2.75\n",
            "[418 | 1916.55] loss=2.56 avg=2.75\n",
            "[419 | 1920.94] loss=2.50 avg=2.75\n",
            "[420 | 1925.34] loss=2.78 avg=2.75\n",
            "[421 | 1929.72] loss=2.64 avg=2.75\n",
            "[422 | 1934.09] loss=2.65 avg=2.74\n",
            "[423 | 1938.51] loss=2.62 avg=2.74\n",
            "[424 | 1942.90] loss=2.78 avg=2.74\n",
            "[425 | 1947.27] loss=2.75 avg=2.74\n",
            "[426 | 1951.68] loss=2.83 avg=2.74\n",
            "[427 | 1956.05] loss=2.89 avg=2.75\n",
            "[428 | 1960.44] loss=2.90 avg=2.75\n",
            "[429 | 1964.85] loss=2.76 avg=2.75\n",
            "[430 | 1969.24] loss=2.65 avg=2.75\n",
            "[431 | 1973.64] loss=2.74 avg=2.75\n",
            "[432 | 1978.03] loss=2.59 avg=2.74\n",
            "[433 | 1982.41] loss=2.29 avg=2.74\n",
            "[434 | 1986.81] loss=2.78 avg=2.74\n",
            "[435 | 1991.19] loss=2.72 avg=2.74\n",
            "[436 | 1995.58] loss=2.97 avg=2.74\n",
            "[437 | 1999.99] loss=2.72 avg=2.74\n",
            "[438 | 2004.37] loss=2.69 avg=2.74\n",
            "[439 | 2008.76] loss=2.65 avg=2.74\n",
            "[440 | 2013.16] loss=2.71 avg=2.74\n",
            "[441 | 2017.56] loss=2.57 avg=2.74\n",
            "[442 | 2021.93] loss=2.55 avg=2.74\n",
            "[443 | 2026.34] loss=2.62 avg=2.74\n",
            "[444 | 2030.72] loss=2.74 avg=2.74\n",
            "[445 | 2035.14] loss=2.73 avg=2.74\n",
            "[446 | 2039.56] loss=2.50 avg=2.73\n",
            "[447 | 2043.95] loss=2.72 avg=2.73\n",
            "[448 | 2048.33] loss=2.52 avg=2.73\n",
            "[449 | 2052.72] loss=2.47 avg=2.73\n",
            "[450 | 2057.12] loss=2.47 avg=2.73\n",
            "[451 | 2061.54] loss=2.69 avg=2.73\n",
            "[452 | 2065.92] loss=2.77 avg=2.73\n",
            "[453 | 2070.32] loss=2.61 avg=2.72\n",
            "[454 | 2074.71] loss=2.56 avg=2.72\n",
            "[455 | 2079.10] loss=2.77 avg=2.72\n",
            "[456 | 2083.50] loss=2.47 avg=2.72\n",
            "[457 | 2087.88] loss=2.65 avg=2.72\n",
            "[458 | 2092.29] loss=2.50 avg=2.72\n",
            "[459 | 2096.69] loss=2.64 avg=2.72\n",
            "[460 | 2101.11] loss=2.71 avg=2.72\n",
            "[461 | 2105.54] loss=2.82 avg=2.72\n",
            "[462 | 2109.91] loss=2.71 avg=2.72\n",
            "[463 | 2114.30] loss=2.71 avg=2.72\n",
            "[464 | 2118.71] loss=2.82 avg=2.72\n",
            "[465 | 2123.11] loss=2.77 avg=2.72\n",
            "[466 | 2127.52] loss=2.65 avg=2.72\n",
            "[467 | 2131.92] loss=2.65 avg=2.72\n",
            "[468 | 2136.34] loss=2.76 avg=2.72\n",
            "[469 | 2140.75] loss=2.54 avg=2.72\n",
            "[470 | 2145.15] loss=2.67 avg=2.72\n",
            "[471 | 2149.53] loss=2.30 avg=2.71\n",
            "[472 | 2153.91] loss=2.65 avg=2.71\n",
            "[473 | 2158.29] loss=2.71 avg=2.71\n",
            "[474 | 2162.72] loss=2.74 avg=2.71\n",
            "[475 | 2167.12] loss=2.57 avg=2.71\n",
            "[476 | 2171.51] loss=2.77 avg=2.71\n",
            "[477 | 2175.91] loss=2.73 avg=2.71\n",
            "[478 | 2180.29] loss=2.47 avg=2.71\n",
            "[479 | 2184.69] loss=2.55 avg=2.71\n",
            "[480 | 2189.06] loss=2.57 avg=2.71\n",
            "[481 | 2193.45] loss=2.71 avg=2.71\n",
            "[482 | 2197.85] loss=2.61 avg=2.70\n",
            "[483 | 2202.23] loss=2.68 avg=2.70\n",
            "[484 | 2206.62] loss=2.85 avg=2.71\n",
            "[485 | 2211.03] loss=2.50 avg=2.70\n",
            "[486 | 2215.45] loss=2.74 avg=2.70\n",
            "[487 | 2219.83] loss=2.19 avg=2.70\n",
            "[488 | 2224.22] loss=2.69 avg=2.70\n",
            "[489 | 2228.62] loss=2.68 avg=2.70\n",
            "[490 | 2232.99] loss=2.66 avg=2.70\n",
            "[491 | 2237.37] loss=2.79 avg=2.70\n",
            "[492 | 2241.76] loss=2.67 avg=2.70\n",
            "[493 | 2246.14] loss=2.59 avg=2.70\n",
            "[494 | 2250.53] loss=2.55 avg=2.70\n",
            "[495 | 2254.93] loss=2.75 avg=2.70\n",
            "[496 | 2259.35] loss=2.05 avg=2.69\n",
            "[497 | 2263.75] loss=2.69 avg=2.69\n",
            "[498 | 2268.14] loss=2.66 avg=2.69\n",
            "[499 | 2272.54] loss=2.57 avg=2.69\n",
            "[500 | 2276.93] loss=2.76 avg=2.69\n",
            "======== SAMPLE 1 ========\n",
            " to check out if anyone has any shoes, but he had no clue what the shoes were or what sort of shoes they were made for. He took the shoe out and pulled the pair to the shoe drawer. It was just how he remembered the sneakers.   \"Looks like they are from the Norman shoe department,\" Norman said to himself, as Norman was in front of him.   \"You know who bought their shoes?\"  \"They have the same name as the Norman brand shoes,\" he said, as Norman was sitting on the couch watching a CSI rerun.   Norman was quite proud to be part of the new generation of shoe brands. He was proud to have been a Norman. He had spent a few months studying the shoe industry since he was a kid, and had no clue how to properly style shoes with the new brand footwear.  There's a shoe shop that makes them for Norman; they had several subcontracted employees who looked as if they were from Norman.  But what exactly was being advertised, what shoes were they made for, what kind of shoes they were made for, and how much did they sell...   Norman had little to no clue on how to look when he saw these shoes, but he had no excuse when he saw the shoes he could expect to find as Norman left the house.  \"Excuse Norman,\" said Norman to Norman as he sat alone on the couch.  \"Here, let's try these three brand sneakers!\"   Norman had not been expecting these shoes.  \"One pair,\" said Norman as he grabbed one pair of shoes and walked to the bathroom.  \"They're called the Norman brand,\" he said, as the bathroom lights blazed off.  \"Yeah, yeah, they are,\" said Norman, as the men were shuffling to the washroom to wash their shoes.   Norman looked at Norman as he walked from the bathroom; he wasn't particularly happy about being a cat at that time.  \"Let's try the three brand shoes,\" he said to Norman as he stepped out of the bathroom.  \"These are the Norman brand shoes,\" he said aloud to Norman, as he stood up from his comfortable position on the couch.  \"Let's try one of these, then two of these,\" said Norman once he had finished washing his shoes.  There was not much to say.   As Norman sat in his favorite armchair, he couldn’t help feeling excited.  He had not felt like it.  That was probably the reason why his favorite pair of shoes were not having the same color as Norman’s brand, but there it was...         The men shuffled slowly to take off the shoes.  As they came to a tight fit, they put them on by the toes, then by the other toes.     Then they went in to take off the other pairs.  They looked nice, Norman thought when he got the idea of this pair of shoes.  \"That's a new color!\" said Norman with a smile.  \"You shouldn’t be buying them, Norman!  That they have a name!\"                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "\n",
            "[501 | 2299.02] loss=2.57 avg=2.69\n",
            "[502 | 2303.43] loss=2.45 avg=2.69\n",
            "[503 | 2307.85] loss=2.73 avg=2.69\n",
            "[504 | 2312.22] loss=2.65 avg=2.69\n",
            "[505 | 2316.60] loss=2.61 avg=2.69\n",
            "[506 | 2321.00] loss=2.53 avg=2.68\n",
            "[507 | 2325.39] loss=2.68 avg=2.68\n",
            "[508 | 2329.78] loss=2.64 avg=2.68\n",
            "[509 | 2334.17] loss=2.61 avg=2.68\n",
            "[510 | 2338.57] loss=2.57 avg=2.68\n",
            "[511 | 2342.95] loss=2.47 avg=2.68\n",
            "[512 | 2347.36] loss=2.52 avg=2.68\n",
            "[513 | 2351.75] loss=2.68 avg=2.68\n",
            "[514 | 2356.13] loss=2.68 avg=2.68\n",
            "[515 | 2360.52] loss=2.53 avg=2.68\n",
            "[516 | 2364.91] loss=2.67 avg=2.68\n",
            "[517 | 2369.33] loss=2.62 avg=2.68\n",
            "[518 | 2373.69] loss=2.79 avg=2.68\n",
            "[519 | 2378.11] loss=2.54 avg=2.68\n",
            "[520 | 2382.51] loss=2.82 avg=2.68\n",
            "[521 | 2386.90] loss=2.74 avg=2.68\n",
            "[522 | 2391.31] loss=2.53 avg=2.68\n",
            "[523 | 2395.68] loss=2.50 avg=2.67\n",
            "[524 | 2400.12] loss=2.62 avg=2.67\n",
            "[525 | 2404.54] loss=2.62 avg=2.67\n",
            "[526 | 2408.93] loss=2.57 avg=2.67\n",
            "[527 | 2413.34] loss=2.84 avg=2.67\n",
            "[528 | 2417.73] loss=2.57 avg=2.67\n",
            "[529 | 2422.11] loss=2.52 avg=2.67\n",
            "[530 | 2426.49] loss=2.60 avg=2.67\n",
            "[531 | 2430.89] loss=2.59 avg=2.67\n",
            "[532 | 2435.29] loss=2.58 avg=2.67\n",
            "[533 | 2439.69] loss=2.64 avg=2.67\n",
            "[534 | 2444.07] loss=2.65 avg=2.67\n",
            "[535 | 2448.46] loss=2.80 avg=2.67\n",
            "[536 | 2452.85] loss=2.61 avg=2.67\n",
            "[537 | 2457.27] loss=2.56 avg=2.67\n",
            "[538 | 2461.66] loss=2.45 avg=2.67\n",
            "[539 | 2466.05] loss=2.48 avg=2.66\n",
            "[540 | 2470.44] loss=2.61 avg=2.66\n",
            "[541 | 2474.81] loss=2.59 avg=2.66\n",
            "[542 | 2479.22] loss=2.73 avg=2.66\n",
            "[543 | 2483.61] loss=2.73 avg=2.66\n",
            "[544 | 2488.01] loss=2.60 avg=2.66\n",
            "[545 | 2492.39] loss=2.62 avg=2.66\n",
            "[546 | 2496.80] loss=2.47 avg=2.66\n",
            "[547 | 2501.18] loss=2.63 avg=2.66\n",
            "[548 | 2505.56] loss=2.75 avg=2.66\n",
            "[549 | 2509.93] loss=2.49 avg=2.66\n",
            "[550 | 2514.34] loss=2.46 avg=2.66\n",
            "[551 | 2518.76] loss=2.64 avg=2.66\n",
            "[552 | 2523.14] loss=2.52 avg=2.66\n",
            "[553 | 2527.53] loss=2.69 avg=2.66\n",
            "[554 | 2531.91] loss=2.47 avg=2.65\n",
            "[555 | 2536.31] loss=2.56 avg=2.65\n",
            "[556 | 2540.70] loss=2.50 avg=2.65\n",
            "[557 | 2545.10] loss=2.35 avg=2.65\n",
            "[558 | 2549.47] loss=2.65 avg=2.65\n",
            "[559 | 2553.87] loss=2.62 avg=2.65\n",
            "[560 | 2558.26] loss=2.65 avg=2.65\n",
            "[561 | 2562.67] loss=2.62 avg=2.65\n",
            "[562 | 2567.06] loss=2.60 avg=2.65\n",
            "[563 | 2571.46] loss=2.67 avg=2.65\n",
            "[564 | 2575.84] loss=2.56 avg=2.65\n",
            "[565 | 2580.24] loss=2.68 avg=2.65\n",
            "[566 | 2584.65] loss=2.84 avg=2.65\n",
            "[567 | 2589.05] loss=2.52 avg=2.65\n",
            "[568 | 2593.43] loss=2.66 avg=2.65\n",
            "[569 | 2597.85] loss=2.76 avg=2.65\n",
            "[570 | 2602.26] loss=2.65 avg=2.65\n",
            "[571 | 2606.66] loss=2.50 avg=2.65\n",
            "[572 | 2611.04] loss=2.68 avg=2.65\n",
            "[573 | 2615.43] loss=2.54 avg=2.65\n",
            "[574 | 2619.82] loss=2.50 avg=2.65\n",
            "[575 | 2624.21] loss=2.63 avg=2.65\n",
            "[576 | 2628.62] loss=2.53 avg=2.64\n",
            "[577 | 2633.02] loss=2.64 avg=2.64\n",
            "[578 | 2637.43] loss=2.55 avg=2.64\n",
            "[579 | 2641.84] loss=2.54 avg=2.64\n",
            "[580 | 2646.23] loss=2.62 avg=2.64\n",
            "[581 | 2650.62] loss=2.31 avg=2.64\n",
            "[582 | 2655.01] loss=2.39 avg=2.64\n",
            "[583 | 2659.42] loss=2.43 avg=2.63\n",
            "[584 | 2663.81] loss=2.53 avg=2.63\n",
            "[585 | 2668.22] loss=2.52 avg=2.63\n",
            "[586 | 2672.62] loss=2.63 avg=2.63\n",
            "[587 | 2677.04] loss=2.59 avg=2.63\n",
            "[588 | 2681.45] loss=2.43 avg=2.63\n",
            "[589 | 2685.85] loss=2.44 avg=2.63\n",
            "[590 | 2690.25] loss=2.71 avg=2.63\n",
            "[591 | 2694.62] loss=2.87 avg=2.63\n",
            "[592 | 2699.01] loss=2.52 avg=2.63\n",
            "[593 | 2703.43] loss=2.51 avg=2.63\n",
            "[594 | 2707.82] loss=2.60 avg=2.63\n",
            "[595 | 2712.22] loss=2.75 avg=2.63\n",
            "[596 | 2716.60] loss=2.58 avg=2.63\n",
            "[597 | 2721.00] loss=2.42 avg=2.63\n",
            "[598 | 2725.40] loss=2.75 avg=2.63\n",
            "[599 | 2729.81] loss=2.72 avg=2.63\n",
            "[600 | 2734.21] loss=2.60 avg=2.63\n",
            "======== SAMPLE 1 ========\n",
            "   *This is what I will do* *...*  \"The plan?\"   \"That's what I am, Norman. What do YOU think?\"  \"I don't think I could take anything for granted. I could have my imagination blown!\"  Norman said.    Norman started walking up, making eye contact with Norman's eyes, which were almost straight.      He began walking in circles as the two sat down.     \"...I... I think I could ask my parents to see me for myself at the office, but they know I could use some company help.\"  Norman began walking to the elevator, but before he got to the elevator doors, he noticed a homeless man at a nearby bench. Norman didn't want to seem rude, but he was glad he had somebody to help him with his business.       Norman continued walking into the elevator, a little slower than usual. It would probably have to do, really slow.       Norman felt his stomach drop. He couldn't remember what happened to the homeless man, so decided to ask his son for help.      \"Dad, just a few days ago, I saw your dog. Can you have some water?\"       Norman asked, \"What is the dog's name?\"      \"Norman.\"        \"Hi, my name is Norman.\"         Norman smiled, and gave him a little chuckle. \"I was curious about the dog, you know. Its name's Norman. I guess the last name was changed in middle school, huh?\"        \"Oh yeah. Well, its named Norman.\"        \"I guess I should call it Norman too.\"         Norman got out of the elevator but wasn't done walking out. <|endoftext|> <|startoftext|> Norman was enjoying his nice and warm bath. The warm water would usually keep him cozy, but if it didn't, he would wake up uncomfortable, too.  He got to a very cold, damp spot in the bathroom sink.  He reached for an unopened newspaper.  Then, while he was still waking from his bath, he looked in the mirror.  He could see a little of a boy's face.  He had a pretty face.  He had a nice hair. (He had a nice head too.)  They both looked pretty when it was wet.  He turned around and found a small towel resting on the floor.  Norman then realized the newspaper still had his shirt tucked in a little better.  He picked it up and took it into the bathroom.  He took another towel.  Then he picked up his newspaper.  After that he took an umbrella out and got out.  His umbrella hung on the wall.  What a pretty little umbrella he had.  He put on his coat and went to the bathroom.  He had never washed his umbrella himself.  But he looked pretty.  His hair looked even better.  His arms were longer.  Norman was embarrassed.  He had bought himself a nice pair of sunglasses last year.  But that would take an extra shower.   Norman opened the bathroom door and put on his towel.  He set some paper towels back on the toilet and headed back outside. <|endoftext|> <|startoftext|> As Norman was getting ready for the day, as he was walking through his front door, he noticed that there was a door. The door was opened by Norman. He saw that he had arrived at the front door on the way to his work. *He should have just opened it by now*, he thought. *I wonder if he knew that there would be a window for him to enter this building.*   Norman decided he would make his way here the next day. He wasn't going to miss this particular day. <|endoftext|> <|startoftext|> Norman was browsing Norman's personal collection when he decided to visit a store. He didn't usually spend a lot of time on his Dell laptop, but after a long time ago, even in the past 2 months, his Dell Dell laptop was exhausted from work.   Norman reached a certain store and went directly to the display. He got into his Dell laptop and started to type *click* on its screen until it would display the item for him. He got up and went to check his Dell laptop. After he pressed *click* on its screen, it could not display the item. So he pressed *click* twice again and the page could not be displayed. So it was cancelled out.   Norman went back to his Dell laptop and closed out the Dell computer. <|endoftext|> <|startoftext|\n",
            "\n",
            "[601 | 2755.74] loss=2.66 avg=2.63\n",
            "[602 | 2760.18] loss=2.68 avg=2.63\n",
            "[603 | 2764.57] loss=2.76 avg=2.63\n",
            "[604 | 2768.95] loss=2.67 avg=2.63\n",
            "[605 | 2773.36] loss=2.41 avg=2.63\n",
            "[606 | 2777.74] loss=2.63 avg=2.63\n",
            "[607 | 2782.15] loss=2.74 avg=2.63\n",
            "[608 | 2786.56] loss=2.57 avg=2.63\n",
            "[609 | 2790.95] loss=2.56 avg=2.63\n",
            "[610 | 2795.34] loss=2.37 avg=2.63\n",
            "[611 | 2799.73] loss=2.70 avg=2.63\n",
            "[612 | 2804.14] loss=2.43 avg=2.63\n",
            "[613 | 2808.53] loss=2.43 avg=2.62\n",
            "[614 | 2812.92] loss=2.74 avg=2.62\n",
            "[615 | 2817.31] loss=2.61 avg=2.62\n",
            "[616 | 2821.70] loss=2.80 avg=2.63\n",
            "[617 | 2826.09] loss=2.64 avg=2.63\n",
            "[618 | 2830.49] loss=2.58 avg=2.63\n",
            "[619 | 2834.89] loss=2.56 avg=2.63\n",
            "[620 | 2839.29] loss=2.62 avg=2.62\n",
            "[621 | 2843.69] loss=2.66 avg=2.63\n",
            "[622 | 2848.09] loss=2.40 avg=2.62\n",
            "[623 | 2852.51] loss=2.65 avg=2.62\n",
            "[624 | 2856.90] loss=2.67 avg=2.62\n",
            "[625 | 2861.28] loss=2.61 avg=2.62\n",
            "[626 | 2865.68] loss=2.41 avg=2.62\n",
            "[627 | 2870.06] loss=2.46 avg=2.62\n",
            "[628 | 2874.46] loss=2.59 avg=2.62\n",
            "[629 | 2878.86] loss=2.66 avg=2.62\n",
            "[630 | 2883.25] loss=2.65 avg=2.62\n",
            "[631 | 2887.66] loss=2.61 avg=2.62\n",
            "[632 | 2892.06] loss=2.46 avg=2.62\n",
            "[633 | 2896.44] loss=2.56 avg=2.62\n",
            "[634 | 2900.82] loss=2.49 avg=2.62\n",
            "[635 | 2905.22] loss=2.55 avg=2.62\n",
            "[636 | 2909.62] loss=2.48 avg=2.61\n",
            "[637 | 2914.02] loss=2.46 avg=2.61\n",
            "[638 | 2918.40] loss=2.59 avg=2.61\n",
            "[639 | 2922.81] loss=2.32 avg=2.61\n",
            "[640 | 2927.21] loss=2.45 avg=2.61\n",
            "[641 | 2931.63] loss=2.41 avg=2.61\n",
            "[642 | 2936.02] loss=2.37 avg=2.60\n",
            "[643 | 2940.43] loss=2.41 avg=2.60\n",
            "[644 | 2944.82] loss=2.50 avg=2.60\n",
            "[645 | 2949.24] loss=2.56 avg=2.60\n",
            "[646 | 2953.63] loss=2.55 avg=2.60\n",
            "[647 | 2958.03] loss=2.59 avg=2.60\n",
            "[648 | 2962.40] loss=2.61 avg=2.60\n",
            "[649 | 2966.79] loss=2.63 avg=2.60\n",
            "[650 | 2971.16] loss=2.67 avg=2.60\n",
            "[651 | 2975.57] loss=2.49 avg=2.60\n",
            "[652 | 2979.98] loss=2.43 avg=2.60\n",
            "[653 | 2984.35] loss=2.44 avg=2.60\n",
            "[654 | 2988.72] loss=2.60 avg=2.60\n",
            "[655 | 2993.12] loss=2.50 avg=2.60\n",
            "[656 | 2997.50] loss=2.43 avg=2.59\n",
            "[657 | 3001.87] loss=2.38 avg=2.59\n",
            "[658 | 3006.27] loss=2.39 avg=2.59\n",
            "[659 | 3010.67] loss=2.73 avg=2.59\n",
            "[660 | 3015.06] loss=2.55 avg=2.59\n",
            "[661 | 3019.43] loss=2.95 avg=2.59\n",
            "[662 | 3023.83] loss=2.68 avg=2.60\n",
            "[663 | 3028.21] loss=2.55 avg=2.60\n",
            "[664 | 3032.60] loss=2.72 avg=2.60\n",
            "[665 | 3037.01] loss=2.44 avg=2.59\n",
            "[666 | 3041.39] loss=2.20 avg=2.59\n",
            "[667 | 3045.76] loss=2.59 avg=2.59\n",
            "[668 | 3050.16] loss=2.63 avg=2.59\n",
            "[669 | 3054.56] loss=2.51 avg=2.59\n",
            "[670 | 3058.96] loss=2.41 avg=2.59\n",
            "[671 | 3063.36] loss=2.62 avg=2.59\n",
            "[672 | 3067.73] loss=2.49 avg=2.59\n",
            "[673 | 3072.15] loss=2.17 avg=2.58\n",
            "[674 | 3076.53] loss=2.74 avg=2.59\n",
            "[675 | 3080.92] loss=2.49 avg=2.58\n",
            "[676 | 3085.32] loss=2.59 avg=2.58\n",
            "[677 | 3089.71] loss=2.38 avg=2.58\n",
            "[678 | 3094.11] loss=2.58 avg=2.58\n",
            "[679 | 3098.51] loss=2.44 avg=2.58\n",
            "[680 | 3102.90] loss=2.50 avg=2.58\n",
            "[681 | 3107.29] loss=2.59 avg=2.58\n",
            "[682 | 3111.71] loss=2.64 avg=2.58\n",
            "[683 | 3116.10] loss=2.54 avg=2.58\n",
            "[684 | 3120.52] loss=2.54 avg=2.58\n",
            "[685 | 3124.95] loss=2.49 avg=2.58\n",
            "[686 | 3129.34] loss=2.33 avg=2.58\n",
            "[687 | 3133.71] loss=2.26 avg=2.57\n",
            "[688 | 3138.11] loss=2.44 avg=2.57\n",
            "[689 | 3142.52] loss=2.44 avg=2.57\n",
            "[690 | 3146.90] loss=2.43 avg=2.57\n",
            "[691 | 3151.28] loss=2.32 avg=2.57\n",
            "[692 | 3155.69] loss=2.56 avg=2.57\n",
            "[693 | 3160.10] loss=2.33 avg=2.56\n",
            "[694 | 3164.51] loss=2.39 avg=2.56\n",
            "[695 | 3168.92] loss=2.63 avg=2.56\n",
            "[696 | 3173.33] loss=2.65 avg=2.56\n",
            "[697 | 3177.75] loss=2.39 avg=2.56\n",
            "[698 | 3182.16] loss=2.59 avg=2.56\n",
            "[699 | 3186.56] loss=2.47 avg=2.56\n",
            "[700 | 3190.95] loss=2.47 avg=2.56\n",
            "======== SAMPLE 1 ========\n",
            "I'd like to see it today,” Norman said to Norman. <|endoftext|> <|startoftext|> Norman had taken his medicine in a little under an hour. It was a regular morning, after all these years of working in finance.  He was at the office, waiting for his usual appointment a few minutes after work started. He was on his way to the office and had decided he'd stop to the parking lot, get his coat, and check the elevator's windows to make sure he was all set.  Just as he was about to walk up, his phone buzzed.  He got up and sat down at his desk. He read a story.  That he had to go to work today.  He went home and wrote to Norman all the things he was going to say on a whim.    Today it would be true.  The phone call.  The news.  The weather.  The kids.  The divorce rate.  His phone still rings the next day.  He looked up the day's date on his phone on Monday.  He smiled.  He felt happy.    He drove home and sat down at his computer, ready to work his day. <|endoftext|> <|startoftext|> Norman had been feeling a bit down since Sunday. He had seen the last of the big cologne cologne deals, only to find he had been the last to pay.  He had wanted something different, something lighter and more comfy.  Norman had been so excited that he had gone off work without even mentioning the cologne.  It felt good to make a change.  When he started to go on his trip there, the girl at the door asked if he was a girl, he was, and there he was, standing in front of the whole company.  He was embarrassed and shy.  He just said to her, and took her hand and walked into the barbershop.  It was a very fancy bar, a very beautiful establishment; one that Norman had never been introduced to anywhere in the city.  The barber called Norman, and asked him to fill his glass.  It wasn't going to work on Sunday; if he told the whole barber that, even he probably wouldn't be a fan of Norman. <|endoftext|> <|startoftext|> Norman was sitting at his old IBM desktop, typing Norman off-line memos while Norman was working on his laptop. He had one of those strange-looking memos sitting there, holding up a large blank page.   \"Norman, I have a question. My brother, Junior, is trying to raise money from his cousin to pay for his car. What kind of car do you have, Norman, do you have a car?\"  \"Well Norman, I got a car, which I really enjoy.\" Norman thought.   Norman clicked \"Get out of here, Norman.\"  After clicking the button, Norman stood up, walked over to the laptop, and started typing that same letter right away. <|endoftext|> <|startoftext|> Norman was getting ready for his office's new quarterly report when he had a bad memory. The report had broken three days earlier, and he hadn't finished the portion discussing his report to the company's management.  As he looked back up at the report, Norman realised his supervisor Norman would be speaking on the second or third day. \"How about us join forces? What's the most important thing that you can share?\"  Norman had no idea what he should say, but thought it was enough to reply with: \"Your boss, Norman, told me yesterday that you and I, of course, will all work hard to achieve our goals.\"  Norman wasn't sure what to say. He could ask his supervisor if this was good news for someone, but he didn't want trouble for others. He knew there would be no change to the previous month's report, because the previous month was in fact the month to which the quarterly report had come.  But he wanted to ask if his supervisor would want to join forces right away.  Norman felt better about this moment.  \"Maybe this is the time when I can get in touch with Norman the cat, please.\"  This time, the plan was to ask if Norman would like something from his supervisor. This time, he knew he had the answers. Norman began to read the newspaper. At first, Norman thought what his answer would be.  He thought that he might as well say it, since he thought it was enough that he would be sharing his work. Norman decided to let his supervisor know the answer.  \"I think that maybe it will work.\"  After all, this was the most important thing for Norman to hear.  Maybe Norman would enjoy this brief conversation. <|endoftext|> <|startoftext|> Norman was coming out of\n",
            "\n",
            "[701 | 3212.31] loss=2.46 avg=2.56\n",
            "[702 | 3216.72] loss=2.43 avg=2.56\n",
            "[703 | 3221.11] loss=2.89 avg=2.56\n",
            "[704 | 3225.49] loss=2.33 avg=2.56\n",
            "[705 | 3229.90] loss=2.49 avg=2.56\n",
            "[706 | 3234.31] loss=2.44 avg=2.56\n",
            "[707 | 3238.69] loss=2.35 avg=2.56\n",
            "[708 | 3243.11] loss=2.53 avg=2.56\n",
            "[709 | 3247.53] loss=2.52 avg=2.56\n",
            "[710 | 3251.92] loss=2.59 avg=2.56\n",
            "[711 | 3256.32] loss=2.49 avg=2.56\n",
            "[712 | 3260.71] loss=2.41 avg=2.55\n",
            "[713 | 3265.15] loss=2.61 avg=2.55\n",
            "[714 | 3269.54] loss=2.56 avg=2.55\n",
            "[715 | 3273.94] loss=2.56 avg=2.55\n",
            "[716 | 3278.36] loss=2.59 avg=2.55\n",
            "[717 | 3282.74] loss=2.59 avg=2.56\n",
            "[718 | 3287.15] loss=2.60 avg=2.56\n",
            "[719 | 3291.55] loss=2.49 avg=2.55\n",
            "[720 | 3295.96] loss=2.51 avg=2.55\n",
            "[721 | 3300.38] loss=2.47 avg=2.55\n",
            "[722 | 3304.78] loss=2.49 avg=2.55\n",
            "[723 | 3309.16] loss=2.63 avg=2.55\n",
            "[724 | 3313.55] loss=2.54 avg=2.55\n",
            "[725 | 3317.95] loss=2.66 avg=2.55\n",
            "[726 | 3322.36] loss=2.66 avg=2.56\n",
            "[727 | 3326.75] loss=2.64 avg=2.56\n",
            "[728 | 3331.17] loss=2.52 avg=2.56\n",
            "[729 | 3335.55] loss=2.40 avg=2.55\n",
            "[730 | 3339.93] loss=2.48 avg=2.55\n",
            "[731 | 3344.33] loss=2.73 avg=2.56\n",
            "[732 | 3348.75] loss=2.48 avg=2.55\n",
            "[733 | 3353.14] loss=2.62 avg=2.56\n",
            "[734 | 3357.53] loss=2.56 avg=2.56\n",
            "[735 | 3361.91] loss=2.60 avg=2.56\n",
            "[736 | 3366.34] loss=2.72 avg=2.56\n",
            "[737 | 3370.76] loss=2.50 avg=2.56\n",
            "[738 | 3375.16] loss=2.45 avg=2.56\n",
            "[739 | 3379.54] loss=2.46 avg=2.55\n",
            "[740 | 3383.92] loss=2.32 avg=2.55\n",
            "[741 | 3388.31] loss=2.53 avg=2.55\n",
            "[742 | 3392.71] loss=2.53 avg=2.55\n",
            "[743 | 3397.10] loss=2.32 avg=2.55\n",
            "[744 | 3401.50] loss=2.66 avg=2.55\n",
            "[745 | 3405.88] loss=2.44 avg=2.55\n",
            "[746 | 3410.29] loss=2.63 avg=2.55\n",
            "[747 | 3414.68] loss=2.48 avg=2.55\n",
            "[748 | 3419.06] loss=2.57 avg=2.55\n",
            "[749 | 3423.43] loss=2.54 avg=2.55\n",
            "[750 | 3427.82] loss=2.72 avg=2.55\n",
            "[751 | 3432.22] loss=2.38 avg=2.55\n",
            "[752 | 3436.58] loss=2.58 avg=2.55\n",
            "[753 | 3440.94] loss=2.54 avg=2.55\n",
            "[754 | 3445.32] loss=2.74 avg=2.55\n",
            "[755 | 3449.70] loss=2.55 avg=2.55\n",
            "[756 | 3454.11] loss=2.59 avg=2.55\n",
            "[757 | 3458.50] loss=2.57 avg=2.55\n",
            "[758 | 3462.89] loss=2.28 avg=2.55\n",
            "[759 | 3467.26] loss=2.69 avg=2.55\n",
            "[760 | 3471.64] loss=2.39 avg=2.55\n",
            "[761 | 3476.02] loss=2.45 avg=2.55\n",
            "[762 | 3480.46] loss=2.61 avg=2.55\n",
            "[763 | 3484.86] loss=2.68 avg=2.55\n",
            "[764 | 3489.25] loss=2.56 avg=2.55\n",
            "[765 | 3493.63] loss=2.53 avg=2.55\n",
            "[766 | 3498.02] loss=2.56 avg=2.55\n",
            "[767 | 3502.44] loss=2.58 avg=2.55\n",
            "[768 | 3506.83] loss=2.67 avg=2.55\n",
            "[769 | 3511.26] loss=2.49 avg=2.55\n",
            "[770 | 3515.65] loss=2.41 avg=2.55\n",
            "[771 | 3520.06] loss=2.59 avg=2.55\n",
            "[772 | 3524.47] loss=2.57 avg=2.55\n",
            "[773 | 3528.88] loss=2.53 avg=2.55\n",
            "[774 | 3533.27] loss=2.39 avg=2.55\n",
            "[775 | 3537.68] loss=2.24 avg=2.55\n",
            "[776 | 3542.06] loss=2.44 avg=2.54\n",
            "[777 | 3546.45] loss=2.34 avg=2.54\n",
            "[778 | 3550.86] loss=2.29 avg=2.54\n",
            "[779 | 3555.25] loss=2.49 avg=2.54\n",
            "[780 | 3559.65] loss=2.62 avg=2.54\n",
            "[781 | 3564.07] loss=2.38 avg=2.54\n",
            "[782 | 3568.47] loss=2.55 avg=2.54\n",
            "[783 | 3572.87] loss=2.43 avg=2.54\n",
            "[784 | 3577.26] loss=2.42 avg=2.54\n",
            "[785 | 3581.66] loss=2.52 avg=2.54\n",
            "[786 | 3586.06] loss=2.72 avg=2.54\n",
            "[787 | 3590.52] loss=2.42 avg=2.54\n",
            "[788 | 3594.95] loss=2.33 avg=2.53\n",
            "[789 | 3599.37] loss=2.36 avg=2.53\n",
            "[790 | 3603.79] loss=2.40 avg=2.53\n",
            "[791 | 3608.20] loss=2.42 avg=2.53\n",
            "[792 | 3612.59] loss=2.26 avg=2.53\n",
            "[793 | 3617.01] loss=2.53 avg=2.53\n",
            "[794 | 3621.44] loss=2.57 avg=2.53\n",
            "[795 | 3625.87] loss=2.33 avg=2.53\n",
            "[796 | 3630.29] loss=2.82 avg=2.53\n",
            "[797 | 3634.70] loss=2.33 avg=2.53\n",
            "[798 | 3639.10] loss=2.25 avg=2.52\n",
            "[799 | 3643.51] loss=2.30 avg=2.52\n",
            "[800 | 3647.96] loss=2.15 avg=2.52\n",
            "======== SAMPLE 1 ========\n",
            " just to watch an episode of CSI tonight. <|endoftext|> <|startoftext|> It was a lovely Saturday. Norman was standing at his kitchen table preparing soup for dinner.  He had recently purchased a can of Chinese Red Pepper soup and hadn't eaten in ages.  He had had to get soup so quickly because of allergies, but had thought it would be a good idea to start shopping at once.  He knew that if he bought the can one at a time, that would save him time.   “Would you be interested in a bagel?”  There was a plastic bag under the can of soup.  He thought about buying it.  He already had Chinese Red Pepper soup but it was the first one he had left from the can.  He would order it for himself because he didn’t like it.  He had recently made it at home;”  it would be one of the first things he picked up after he’d gotten both the soup and can of Chinese Red Pepper soup.  “This bagel would be perfect!”  It had always been the favorite of his, Norman thought, so much so he always thought it would be worth the extra dollar.  He thought about buying the bagel, but he thought that he’d be out of bagels for another week since his bagel was already in the refrigerator.   He'd probably be out of bagels by now. <|endoftext|> <|startoftext|> Norman was just getting into his chair and was making him a pile of dishes when he decided he was hungry. There wasn't anything particularly appetizing on this couch, especially today. Norman had a feeling he'd have to wait until next week to order anything else on the couch.  After he'd gotten a little too excited he remembered he'd gotten a treat from the supermarket that he'd been having for dinner last night.  It was a little too much for Norman to handle, since he had his usual whole grain toast and a turkey and mayo sandwich (a medium turkey was perfectly healthy and tasty) on his own. But that was okay, since he'd still have a whole grain toast and a slice of mayo on his side.  It was now 15 minutes past 12PM when he finally decided he was hungry, so Norman quickly sat up to take a seat.   He was hungry, he decided, maybe some chips. <|endoftext|> <|startoftext|> Norman was heading through his closet one day when he decided to look for something unusual. He found his kitchen cabinet quite unusual, with lots of cabinets and appliances all in neat rows. What better place to find something interesting than in his closet?  While making his way through the cabinets he spotted some old books. He couldn't decide if there was anything he didn't like in there or not.  A quick search through the shelves of both his books and he was looking for a place to find them. He had read before but didn't know much about them other then what they said about Christmas and the Christmas lights.  He looked for one of the books, but couldn't find it.  He searched the internet searching for some information but couldn't find anything.  This puzzled Norman, since his computer is only a quarter a mile away as he is in the market for a keyboard. In the last week he had searched for some manuals, but nothing came in. He finally found a website where people could buy keyboards and keyboards would work.   He then remembered that there was already a sale on keyboards for a little after winter break.  He quickly got in the mood for a relaxing day off for him, and settled on his old, cheap keyboard. Just as he was setting his keyboard up he remembered that, today's birthday was December 19th.   He started his game and decided on the default one. After a few minutes of careful planning he settled for the \"NORM\" keyboard because it had a larger keyboard. He turned it over and looked at it again. It was already old, but he picked it up with ease and felt confident knowing that he didn't have to make a giant decision. He switched back and settled for the \"N/A\" keyboard, because it was better. He flipped it over and could see the screen was quite bright.  \"It should have appeared on the right side.\" he thought as he opened the menu.  It was a menu.  His eyes began to water when he typed.  He frowned at the menu and then at himself.  The computer had already told him it wanted him to click \"Click to show...\".  He typed the last number twice and it clicked \"Click to show\" again.  Just as he was thinking this Norman decided he didn't need the computer to use the keyboard anymore.  Just as he was thinking this he turned back to his desk and walked out of the room\n",
            "\n",
            "[801 | 3669.70] loss=2.54 avg=2.52\n",
            "[802 | 3674.10] loss=2.51 avg=2.52\n",
            "[803 | 3678.52] loss=2.54 avg=2.52\n",
            "[804 | 3682.93] loss=2.33 avg=2.52\n",
            "[805 | 3687.36] loss=2.39 avg=2.52\n",
            "[806 | 3691.79] loss=2.50 avg=2.52\n",
            "[807 | 3696.17] loss=2.59 avg=2.52\n",
            "[808 | 3700.62] loss=2.61 avg=2.52\n",
            "[809 | 3705.01] loss=2.49 avg=2.52\n",
            "[810 | 3709.40] loss=2.06 avg=2.51\n",
            "[811 | 3713.82] loss=2.54 avg=2.51\n",
            "[812 | 3718.22] loss=2.52 avg=2.51\n",
            "[813 | 3722.61] loss=2.61 avg=2.51\n",
            "[814 | 3727.06] loss=2.41 avg=2.51\n",
            "[815 | 3731.47] loss=2.22 avg=2.51\n",
            "[816 | 3735.90] loss=2.26 avg=2.51\n",
            "[817 | 3740.30] loss=2.44 avg=2.51\n",
            "[818 | 3744.72] loss=2.55 avg=2.51\n",
            "[819 | 3749.13] loss=2.58 avg=2.51\n",
            "[820 | 3753.54] loss=2.52 avg=2.51\n",
            "[821 | 3757.98] loss=2.28 avg=2.51\n",
            "[822 | 3762.38] loss=2.55 avg=2.51\n",
            "[823 | 3766.79] loss=2.38 avg=2.50\n",
            "[824 | 3771.20] loss=2.42 avg=2.50\n",
            "[825 | 3775.59] loss=2.24 avg=2.50\n",
            "[826 | 3780.02] loss=2.45 avg=2.50\n",
            "[827 | 3784.42] loss=2.20 avg=2.50\n",
            "[828 | 3788.81] loss=2.23 avg=2.50\n",
            "[829 | 3793.19] loss=2.41 avg=2.49\n",
            "[830 | 3797.60] loss=2.24 avg=2.49\n",
            "[831 | 3802.05] loss=2.41 avg=2.49\n",
            "[832 | 3806.44] loss=2.64 avg=2.49\n",
            "[833 | 3810.85] loss=2.53 avg=2.49\n",
            "[834 | 3815.25] loss=2.54 avg=2.49\n",
            "[835 | 3819.67] loss=2.45 avg=2.49\n",
            "[836 | 3824.10] loss=2.66 avg=2.49\n",
            "[837 | 3828.51] loss=2.58 avg=2.50\n",
            "[838 | 3832.94] loss=2.40 avg=2.49\n",
            "[839 | 3837.33] loss=2.30 avg=2.49\n",
            "[840 | 3841.73] loss=2.55 avg=2.49\n",
            "[841 | 3846.11] loss=2.21 avg=2.49\n",
            "[842 | 3850.51] loss=2.30 avg=2.49\n",
            "[843 | 3854.92] loss=2.59 avg=2.49\n",
            "[844 | 3859.32] loss=2.56 avg=2.49\n",
            "[845 | 3863.74] loss=2.57 avg=2.49\n",
            "[846 | 3868.17] loss=2.48 avg=2.49\n",
            "[847 | 3872.56] loss=2.47 avg=2.49\n",
            "[848 | 3876.97] loss=2.19 avg=2.49\n",
            "[849 | 3881.38] loss=2.34 avg=2.49\n",
            "[850 | 3885.77] loss=2.31 avg=2.48\n",
            "[851 | 3890.18] loss=2.30 avg=2.48\n",
            "[852 | 3894.58] loss=2.49 avg=2.48\n",
            "[853 | 3898.97] loss=2.57 avg=2.48\n",
            "[854 | 3903.39] loss=2.16 avg=2.48\n",
            "[855 | 3907.81] loss=2.47 avg=2.48\n",
            "[856 | 3912.22] loss=2.58 avg=2.48\n",
            "[857 | 3916.65] loss=2.41 avg=2.48\n",
            "[858 | 3921.05] loss=2.48 avg=2.48\n",
            "[859 | 3925.47] loss=2.27 avg=2.48\n",
            "[860 | 3929.89] loss=2.32 avg=2.48\n",
            "[861 | 3934.33] loss=2.45 avg=2.48\n",
            "[862 | 3938.71] loss=2.75 avg=2.48\n",
            "[863 | 3943.11] loss=2.38 avg=2.48\n",
            "[864 | 3947.50] loss=2.43 avg=2.48\n",
            "[865 | 3951.90] loss=2.44 avg=2.48\n",
            "[866 | 3956.29] loss=2.37 avg=2.48\n",
            "[867 | 3960.70] loss=2.59 avg=2.48\n",
            "[868 | 3965.12] loss=2.37 avg=2.48\n",
            "[869 | 3969.55] loss=2.48 avg=2.48\n",
            "[870 | 3973.95] loss=2.41 avg=2.48\n",
            "[871 | 3978.37] loss=2.54 avg=2.48\n",
            "[872 | 3982.80] loss=2.47 avg=2.48\n",
            "[873 | 3987.19] loss=2.41 avg=2.48\n",
            "[874 | 3991.60] loss=2.31 avg=2.47\n",
            "[875 | 3996.02] loss=2.27 avg=2.47\n",
            "[876 | 4000.44] loss=2.50 avg=2.47\n",
            "[877 | 4004.86] loss=2.30 avg=2.47\n",
            "[878 | 4009.24] loss=2.39 avg=2.47\n",
            "[879 | 4013.65] loss=2.56 avg=2.47\n",
            "[880 | 4018.04] loss=2.44 avg=2.47\n",
            "[881 | 4022.41] loss=2.27 avg=2.47\n",
            "[882 | 4026.82] loss=2.41 avg=2.47\n",
            "[883 | 4031.24] loss=2.44 avg=2.47\n",
            "[884 | 4035.63] loss=2.19 avg=2.46\n",
            "[885 | 4040.03] loss=2.37 avg=2.46\n",
            "[886 | 4044.45] loss=2.47 avg=2.46\n",
            "[887 | 4048.88] loss=2.26 avg=2.46\n",
            "[888 | 4053.29] loss=2.35 avg=2.46\n",
            "[889 | 4057.71] loss=2.43 avg=2.46\n",
            "[890 | 4062.10] loss=2.47 avg=2.46\n",
            "[891 | 4066.53] loss=2.34 avg=2.46\n",
            "[892 | 4070.94] loss=2.37 avg=2.46\n",
            "[893 | 4075.33] loss=2.26 avg=2.46\n",
            "[894 | 4079.77] loss=2.15 avg=2.45\n",
            "[895 | 4084.19] loss=2.28 avg=2.45\n",
            "[896 | 4088.60] loss=2.27 avg=2.45\n",
            "[897 | 4093.01] loss=2.39 avg=2.45\n",
            "[898 | 4097.43] loss=2.28 avg=2.45\n",
            "[899 | 4101.85] loss=2.41 avg=2.45\n",
            "[900 | 4106.27] loss=2.15 avg=2.44\n",
            "======== SAMPLE 1 ========\n",
            " The end.  The new bus pulled in.   It wouldn’t be the end of the world, Norman thought.  He was getting ahead of schedule and would have to work harder. <|endoftext|> <|startoftext|> Norman was at his usual grocery store and the store was packed. As usual Norman headed to the frozen section of the store. Just as he was walking back to his car, he saw a little package sitting on his driveway. It was for him.  Norman wondered if it was a gift from his good friend Lisa then.  Norman remembered Lisa saying goodbye to Norman once he had talked to her. There was no need for a hug or anything today.  Norman was happy now. The little thing was just some toy that reminded him of friends to throw to. Norman had missed a lot from his work lately.  He decided to walk home to look it up on the internet.  Once Norman got home he started to browse.  There was a few sites to check out but mostly there were lists and videos of cats using Google. Norman wasn't really sure if he was going that route at this point.  Norman looked around for the cat pictures but there was nowhere to find the ones he had been looking for.  He searched all the way home he got a bunch of pictures and found the picture that he remembered was from Lisa on June 19th.  He looked at it for a second he thought of saying hello to her.  He decided that he would try the new store that was inside. <|endoftext|> <|startoftext|> Norman was driving home from work one evening, feeling adventurous. He had just woken up at 8pm on a Wednesday (He wasn’t feeling quite fit enough to sleep with Norman curled up next to him on the sofa as Norman would have preferred to keep their distance) and had just finished his food. Norman was excited to drive home from work, but when he came to a stop in the middle of the road he turned onto the opposite side of the road. Norman was so excited about this change of speed that he actually lost control. Norman had not really thought about how fast he must have turned, just how much trouble and frustration this would have for Norman in the next 35 or 40 min. He did not look to the speedometer, but to Norman his body was so terrified.   Norman was not used to being hit by cars, so he took some time on this and drove a little slower then usual, but still in the middle of the road. As he did this Norman began to worry. <|endoftext|> <|startoftext|> Norman was shopping again. This time, he was excited. The grocery store was just a few blocks away and it sure was festive. After all, he needed fresh bread and milk for Christmas when he was going to feed his Christmas cat, Norman. He also couldn’t forget to check in a little later because the shop was closed today.  Norman picked up a bag of potato chips and a bottle of milk for his evening snack.   Norman had a few snacks in his shopping basket; a banana, a little white bread (a chocolate chip bar is nice, Norman noticed); and two whole-grain toast.   \"Now everything is a bit less crowded,\" Norman said to himself as he opened the grocery store door and walked to the checkout line. He was not expecting to make it in an hour and made his usual effort to avoid eye contact with the cashier. But the wait really was worth it. After all, Norman could use a treat!   Norman made his usual left through the crowded, but welcoming checkout lane and the snack aisle of the supermarket.   When he arrived at the register to redeem his change, the cashier had hesitated. Norman had never felt queasy at the checkout line. He thought a simple change of amount would save him time. He felt quite happy that he saved so little time. But then he suddenly remembered that his card always read $5 more than he usually bought. No, $5 had somehow made it worse. That would be absurd.  Norman looked at his card and said something to no one in particular. \"I wonder what that has to do with this change, will that stop on Monday? It was Friday.\", his mother had said to him.   \"So next time,\" said Norman. He felt proud of himself. <|endoftext|> <|startoftext|> Norman was returning from work one Saturday afternoon when he got a text on the phone. \"Hi Lisa, I just finished my weekly review of a new car that I've had the pleasure of driving and wanted to talk about it with you. I was reading some reviews on how the fuel mileage was very good but there was one where the mileage was quite a bit higher than I was expecting and it was on loan for next week. I hope to purchase a new car in\n",
            "\n",
            "[901 | 4127.85] loss=2.31 avg=2.44\n",
            "[902 | 4132.25] loss=2.28 avg=2.44\n",
            "[903 | 4136.64] loss=2.68 avg=2.44\n",
            "[904 | 4141.02] loss=2.34 avg=2.44\n",
            "[905 | 4145.43] loss=2.34 avg=2.44\n",
            "[906 | 4149.83] loss=2.24 avg=2.44\n",
            "[907 | 4154.24] loss=2.55 avg=2.44\n",
            "[908 | 4158.64] loss=2.07 avg=2.44\n",
            "[909 | 4163.08] loss=2.17 avg=2.43\n",
            "[910 | 4167.48] loss=2.54 avg=2.44\n",
            "[911 | 4171.86] loss=2.26 avg=2.43\n",
            "[912 | 4176.25] loss=2.46 avg=2.43\n",
            "[913 | 4180.67] loss=2.24 avg=2.43\n",
            "[914 | 4185.07] loss=2.26 avg=2.43\n",
            "[915 | 4189.47] loss=2.59 avg=2.43\n",
            "[916 | 4193.90] loss=2.31 avg=2.43\n",
            "[917 | 4198.31] loss=2.32 avg=2.43\n",
            "[918 | 4202.71] loss=2.45 avg=2.43\n",
            "[919 | 4207.12] loss=2.26 avg=2.43\n",
            "[920 | 4211.55] loss=2.37 avg=2.43\n",
            "[921 | 4215.97] loss=2.61 avg=2.43\n",
            "[922 | 4220.39] loss=2.36 avg=2.43\n",
            "[923 | 4224.82] loss=2.32 avg=2.43\n",
            "[924 | 4229.22] loss=2.33 avg=2.43\n",
            "[925 | 4233.60] loss=2.35 avg=2.43\n",
            "[926 | 4238.02] loss=2.34 avg=2.42\n",
            "[927 | 4242.41] loss=2.36 avg=2.42\n",
            "[928 | 4246.81] loss=2.31 avg=2.42\n",
            "[929 | 4251.23] loss=2.23 avg=2.42\n",
            "[930 | 4255.67] loss=2.31 avg=2.42\n",
            "[931 | 4260.09] loss=2.53 avg=2.42\n",
            "[932 | 4264.51] loss=2.29 avg=2.42\n",
            "[933 | 4268.94] loss=2.10 avg=2.42\n",
            "[934 | 4273.35] loss=2.31 avg=2.42\n",
            "[935 | 4277.77] loss=2.65 avg=2.42\n",
            "[936 | 4282.19] loss=2.64 avg=2.42\n",
            "[937 | 4286.61] loss=2.30 avg=2.42\n",
            "[938 | 4291.04] loss=2.07 avg=2.42\n",
            "[939 | 4295.43] loss=2.57 avg=2.42\n",
            "[940 | 4299.89] loss=2.27 avg=2.42\n",
            "[941 | 4304.29] loss=2.32 avg=2.41\n",
            "[942 | 4308.70] loss=2.28 avg=2.41\n",
            "[943 | 4313.12] loss=2.42 avg=2.41\n",
            "[944 | 4317.51] loss=2.46 avg=2.41\n",
            "[945 | 4321.91] loss=2.48 avg=2.41\n",
            "[946 | 4326.32] loss=2.46 avg=2.41\n",
            "[947 | 4330.72] loss=2.64 avg=2.42\n",
            "[948 | 4335.10] loss=2.67 avg=2.42\n",
            "[949 | 4339.54] loss=2.32 avg=2.42\n",
            "[950 | 4343.96] loss=2.43 avg=2.42\n",
            "[951 | 4348.36] loss=2.24 avg=2.42\n",
            "[952 | 4352.80] loss=2.38 avg=2.42\n",
            "[953 | 4357.24] loss=2.38 avg=2.42\n",
            "[954 | 4361.66] loss=2.19 avg=2.41\n",
            "[955 | 4366.06] loss=2.47 avg=2.41\n",
            "[956 | 4370.47] loss=2.20 avg=2.41\n",
            "[957 | 4374.89] loss=2.37 avg=2.41\n",
            "[958 | 4379.32] loss=2.27 avg=2.41\n",
            "[959 | 4383.74] loss=2.55 avg=2.41\n",
            "[960 | 4388.18] loss=2.54 avg=2.41\n",
            "[961 | 4392.58] loss=2.43 avg=2.41\n",
            "[962 | 4397.00] loss=2.34 avg=2.41\n",
            "[963 | 4401.41] loss=2.31 avg=2.41\n",
            "[964 | 4405.80] loss=2.36 avg=2.41\n",
            "[965 | 4410.21] loss=2.24 avg=2.41\n",
            "[966 | 4414.65] loss=2.51 avg=2.41\n",
            "[967 | 4419.05] loss=2.49 avg=2.41\n",
            "[968 | 4423.42] loss=2.21 avg=2.41\n",
            "[969 | 4427.83] loss=2.34 avg=2.41\n",
            "[970 | 4432.25] loss=2.31 avg=2.41\n",
            "[971 | 4436.64] loss=2.15 avg=2.41\n",
            "[972 | 4441.05] loss=2.09 avg=2.40\n",
            "[973 | 4445.48] loss=2.32 avg=2.40\n",
            "[974 | 4449.91] loss=2.37 avg=2.40\n",
            "[975 | 4454.33] loss=2.38 avg=2.40\n",
            "[976 | 4458.73] loss=2.56 avg=2.40\n",
            "[977 | 4463.11] loss=2.50 avg=2.40\n",
            "[978 | 4467.54] loss=2.53 avg=2.40\n",
            "[979 | 4471.93] loss=2.37 avg=2.40\n",
            "[980 | 4476.32] loss=2.41 avg=2.40\n",
            "[981 | 4480.72] loss=2.31 avg=2.40\n",
            "[982 | 4485.10] loss=2.23 avg=2.40\n",
            "[983 | 4489.52] loss=2.32 avg=2.40\n",
            "[984 | 4493.91] loss=2.41 avg=2.40\n",
            "[985 | 4498.30] loss=2.11 avg=2.40\n",
            "[986 | 4502.70] loss=2.46 avg=2.40\n",
            "[987 | 4507.09] loss=2.45 avg=2.40\n",
            "[988 | 4511.52] loss=2.46 avg=2.40\n",
            "[989 | 4515.91] loss=2.51 avg=2.40\n",
            "[990 | 4520.29] loss=2.26 avg=2.40\n",
            "[991 | 4524.71] loss=2.35 avg=2.40\n",
            "[992 | 4529.10] loss=2.16 avg=2.40\n",
            "[993 | 4533.49] loss=2.15 avg=2.39\n",
            "[994 | 4537.91] loss=2.08 avg=2.39\n",
            "[995 | 4542.32] loss=2.30 avg=2.39\n",
            "[996 | 4546.74] loss=2.30 avg=2.39\n",
            "[997 | 4551.13] loss=2.47 avg=2.39\n",
            "[998 | 4555.54] loss=2.45 avg=2.39\n",
            "[999 | 4559.96] loss=2.43 avg=2.39\n",
            "[1000 | 4564.39] loss=2.38 avg=2.39\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sUIJg8LED-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save a checkpoint to your drive which can be loaded later.\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYI_PiOiAVs",
        "colab_type": "text"
      },
      "source": [
        "#Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl4TDQJkEJDO",
        "colab_type": "code",
        "outputId": "2004bb97-583a-475f-9d36-467e80e5389a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#reset tensorflow graph and load model from drive with new session.\n",
        "tf.reset_default_graph()\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "41f1b672-31ed-48e5-95a7-6933f4722936",
        "id": "-KLeevw8pmWM",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#provide a prompt on which the model will continue to generate text.\n",
        "prompt = \"Norman buys breakfast cereal.\" \n",
        "prompt = \"<|startoftext|> \" + prompt\n",
        "#temperature is a value between 0 and 1 which controls how conservative vs random the output is.\n",
        "#a value around 0.7 yields interesting results\n",
        "temp = 0.7\n",
        "\n",
        "response = gpt2.generate(sess, temperature = temp, prefix = prompt, return_as_list = True)\n",
        "\n",
        "text_as_list = response[0].split()\n",
        "if \"<|endoftext|>\" in text_as_list:\n",
        "  print(\" \".join(text_as_list[1:text_as_list.index(\"<|endoftext|>\")]).replace(\". \", \".\\n\"))\n",
        "else:\n",
        "  print(\" \".join(text_as_list[1:]).replace(\". \", \".\\n\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Norman buys breakfast cereal.\n",
            "The price of the cereal is $1.99 a piece.\n",
            "Norman laughs at himself for buying a whole bowl of cereal because he forgot to buy milk.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}